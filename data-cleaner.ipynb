{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbb2fbb2",
   "metadata": {},
   "source": [
    "Importing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54a9cfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57589ba3",
   "metadata": {},
   "source": [
    "trying to get the shape of data and removing unncessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1c8b9cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "book_id                        int64\n",
       "goodreads_book_id              int64\n",
       "authors                       object\n",
       "original_publication_year    float64\n",
       "original_title                object\n",
       "title                         object\n",
       "average_rating               float64\n",
       "ratings_count                  int64\n",
       "work_ratings_count             int64\n",
       "work_text_reviews_count        int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"books.csv\")\n",
    "df_book_tags = pd.read_csv(\"book_tags.csv\")\n",
    "df_tags = pd.read_csv(\"tags.csv\")\n",
    "# \n",
    "df = df.drop([\n",
    "    'work_id', 'isbn', 'isbn13', 'language_code', 'image_url', 'small_image_url', 'best_book_id', 'books_count','ratings_1', 'ratings_2', 'ratings_3', 'ratings_4', 'ratings_5'\n",
    "],axis = 1)\n",
    "df.dtypes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ccf732c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>goodreads_book_id</th>\n",
       "      <th>tag_id</th>\n",
       "      <th>count</th>\n",
       "      <th>tag_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>30574</td>\n",
       "      <td>167697</td>\n",
       "      <td>to-read</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>11305</td>\n",
       "      <td>37174</td>\n",
       "      <td>fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>11557</td>\n",
       "      <td>34173</td>\n",
       "      <td>favorites</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>8717</td>\n",
       "      <td>12986</td>\n",
       "      <td>currently-reading</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>33114</td>\n",
       "      <td>12716</td>\n",
       "      <td>young-adult</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   goodreads_book_id  tag_id   count           tag_name\n",
       "0                  1   30574  167697            to-read\n",
       "1                  1   11305   37174            fantasy\n",
       "2                  1   11557   34173          favorites\n",
       "3                  1    8717   12986  currently-reading\n",
       "4                  1   33114   12716        young-adult"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tags_named = pd.merge(\n",
    "    df_book_tags,\n",
    "    df_tags,\n",
    "    on='tag_id',\n",
    "    how='left' # Keep all tag records, even if a tag_name is missing (rare)\n",
    ")\n",
    "df_tags_named.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7c86f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated Tags Sample:\n",
      "   goodreads_book_id                                           tag_soup\n",
      "0                  1  fantasy young-adult fiction harry-potter books...\n",
      "1                  2  fantasy children children-s all-time-favorites...\n",
      "2                  3  fantasy young-adult fiction harry-potter books...\n",
      "3                  5  fantasy young-adult fiction harry-potter books...\n",
      "4                  6  fantasy young-adult fiction harry-potter ya se...\n"
     ]
    }
   ],
   "source": [
    "# List of generic tags to exclude (these describe user intent, not book content)\n",
    "GENERIC_TAGS_TO_REMOVE = [\n",
    "    'to-read',\n",
    "    'currently-reading',\n",
    "    'owned',\n",
    "    'favorites',\n",
    "    'default',\n",
    "    'ebook',\n",
    "    'kindle',\n",
    "    'library',\n",
    "    'owned-books',\n",
    "    'my-books',\n",
    "    'books'\n",
    "]\n",
    "\n",
    "# 1. Filter out generic tags\n",
    "df_content_tags = df_tags_named[~df_tags_named['tag_name'].isin(GENERIC_TAGS_TO_REMOVE)].copy()\n",
    "\n",
    "# Sort by count (descending) to prioritize the most important tags for each book\n",
    "df_content_tags = df_content_tags.sort_values(\n",
    "    ['goodreads_book_id', 'count'],\n",
    "    ascending=[True, False]\n",
    ")\n",
    "\n",
    "# 2. Keep only the top 10 most relevant content tags per book (N=10)\n",
    "N_TAGS = 10\n",
    "df_top_tags_per_book = (\n",
    "    df_content_tags.groupby('goodreads_book_id')\n",
    "    .head(N_TAGS)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# Aggregate the top 10 tag names into a single string ('tag_soup')\n",
    "df_tags_grouped = (\n",
    "    df_top_tags_per_book.groupby('goodreads_book_id')['tag_name']\n",
    "    .apply(lambda x: ' '.join(x.astype(str).str.replace(' ', '-'))) # Join, replacing spaces with hyphens for better NLP\n",
    "    .reset_index(name='tag_soup')\n",
    ")\n",
    "\n",
    "print(\"Aggregated Tags Sample:\")\n",
    "print(df_tags_grouped.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b308dc49",
   "metadata": {},
   "source": [
    "Now merging this modified df_tags_grouped table with our main table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d50620a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Final Books Content Data Sample ---\n",
      "   book_id  goodreads_book_id  \\\n",
      "0        1            2767052   \n",
      "1        2                  3   \n",
      "2        3              41865   \n",
      "3        4               2657   \n",
      "4        5               4671   \n",
      "\n",
      "                                               title  \\\n",
      "0            The Hunger Games (The Hunger Games, #1)   \n",
      "1  Harry Potter and the Sorcerer's Stone (Harry P...   \n",
      "2                            Twilight (Twilight, #1)   \n",
      "3                              To Kill a Mockingbird   \n",
      "4                                   The Great Gatsby   \n",
      "\n",
      "                       authors  \\\n",
      "0              Suzanne Collins   \n",
      "1  J.K. Rowling, Mary GrandPré   \n",
      "2              Stephenie Meyer   \n",
      "3                   Harper Lee   \n",
      "4          F. Scott Fitzgerald   \n",
      "\n",
      "                                            tag_soup  \n",
      "0  young-adult fiction dystopian dystopia fantasy...  \n",
      "1  fantasy young-adult fiction harry-potter books...  \n",
      "2  young-adult fantasy vampires ya fiction parano...  \n",
      "3  classics classic historical-fiction school clà...  \n",
      "4  classics fiction classic books-i-own literatur...  \n",
      "\n",
      "Total rows in final dataframe: 10000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# 3. Final Merge with Main Books Table (using the goodreads_book_id)\n",
    "df = pd.merge(\n",
    "    df,\n",
    "    df_tags_grouped,\n",
    "    on='goodreads_book_id',\n",
    "    how='left' \n",
    ")\n",
    "\n",
    "df['tag_soup'] = df['tag_soup'].fillna('')\n",
    "\n",
    "print(\"--- Final Books Content Data Sample ---\")\n",
    "print(df[['book_id', 'goodreads_book_id', 'title', 'authors', 'tag_soup']].head())\n",
    "\n",
    "print(f\"\\nTotal rows in final dataframe: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6adcaede",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book_id</th>\n",
       "      <th>goodreads_book_id</th>\n",
       "      <th>authors</th>\n",
       "      <th>original_publication_year</th>\n",
       "      <th>original_title</th>\n",
       "      <th>title</th>\n",
       "      <th>average_rating</th>\n",
       "      <th>ratings_count</th>\n",
       "      <th>work_ratings_count</th>\n",
       "      <th>work_text_reviews_count</th>\n",
       "      <th>tag_soup</th>\n",
       "      <th>content_soup</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2767052</td>\n",
       "      <td>Suzanne Collins</td>\n",
       "      <td>2008.0</td>\n",
       "      <td>The Hunger Games</td>\n",
       "      <td>The Hunger Games (The Hunger Games, #1)</td>\n",
       "      <td>4.34</td>\n",
       "      <td>4780653</td>\n",
       "      <td>4942365</td>\n",
       "      <td>155254</td>\n",
       "      <td>young-adult fiction dystopian dystopia fantasy...</td>\n",
       "      <td>SuzanneCollins TheHungerGames(TheHungerGames #...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>J.K. Rowling, Mary GrandPré</td>\n",
       "      <td>1997.0</td>\n",
       "      <td>Harry Potter and the Philosopher's Stone</td>\n",
       "      <td>Harry Potter and the Sorcerer's Stone (Harry P...</td>\n",
       "      <td>4.44</td>\n",
       "      <td>4602479</td>\n",
       "      <td>4800065</td>\n",
       "      <td>75867</td>\n",
       "      <td>fantasy young-adult fiction harry-potter books...</td>\n",
       "      <td>J.K.Rowling MaryGrandPré HarryPotterandtheSorc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>41865</td>\n",
       "      <td>Stephenie Meyer</td>\n",
       "      <td>2005.0</td>\n",
       "      <td>Twilight</td>\n",
       "      <td>Twilight (Twilight, #1)</td>\n",
       "      <td>3.57</td>\n",
       "      <td>3866839</td>\n",
       "      <td>3916824</td>\n",
       "      <td>95009</td>\n",
       "      <td>young-adult fantasy vampires ya fiction parano...</td>\n",
       "      <td>StephenieMeyer Twilight(Twilight #1) young-adu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2657</td>\n",
       "      <td>Harper Lee</td>\n",
       "      <td>1960.0</td>\n",
       "      <td>To Kill a Mockingbird</td>\n",
       "      <td>To Kill a Mockingbird</td>\n",
       "      <td>4.25</td>\n",
       "      <td>3198671</td>\n",
       "      <td>3340896</td>\n",
       "      <td>72586</td>\n",
       "      <td>classics classic historical-fiction school clà...</td>\n",
       "      <td>HarperLee ToKillaMockingbird classics classic ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>4671</td>\n",
       "      <td>F. Scott Fitzgerald</td>\n",
       "      <td>1925.0</td>\n",
       "      <td>The Great Gatsby</td>\n",
       "      <td>The Great Gatsby</td>\n",
       "      <td>3.89</td>\n",
       "      <td>2683664</td>\n",
       "      <td>2773745</td>\n",
       "      <td>51992</td>\n",
       "      <td>classics fiction classic books-i-own literatur...</td>\n",
       "      <td>F.ScottFitzgerald TheGreatGatsby classics fict...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   book_id  goodreads_book_id                      authors  \\\n",
       "0        1            2767052              Suzanne Collins   \n",
       "1        2                  3  J.K. Rowling, Mary GrandPré   \n",
       "2        3              41865              Stephenie Meyer   \n",
       "3        4               2657                   Harper Lee   \n",
       "4        5               4671          F. Scott Fitzgerald   \n",
       "\n",
       "   original_publication_year                            original_title  \\\n",
       "0                     2008.0                          The Hunger Games   \n",
       "1                     1997.0  Harry Potter and the Philosopher's Stone   \n",
       "2                     2005.0                                  Twilight   \n",
       "3                     1960.0                     To Kill a Mockingbird   \n",
       "4                     1925.0                          The Great Gatsby   \n",
       "\n",
       "                                               title  average_rating  \\\n",
       "0            The Hunger Games (The Hunger Games, #1)            4.34   \n",
       "1  Harry Potter and the Sorcerer's Stone (Harry P...            4.44   \n",
       "2                            Twilight (Twilight, #1)            3.57   \n",
       "3                              To Kill a Mockingbird            4.25   \n",
       "4                                   The Great Gatsby            3.89   \n",
       "\n",
       "   ratings_count  work_ratings_count  work_text_reviews_count  \\\n",
       "0        4780653             4942365                   155254   \n",
       "1        4602479             4800065                    75867   \n",
       "2        3866839             3916824                    95009   \n",
       "3        3198671             3340896                    72586   \n",
       "4        2683664             2773745                    51992   \n",
       "\n",
       "                                            tag_soup  \\\n",
       "0  young-adult fiction dystopian dystopia fantasy...   \n",
       "1  fantasy young-adult fiction harry-potter books...   \n",
       "2  young-adult fantasy vampires ya fiction parano...   \n",
       "3  classics classic historical-fiction school clà...   \n",
       "4  classics fiction classic books-i-own literatur...   \n",
       "\n",
       "                                        content_soup  \n",
       "0  SuzanneCollins TheHungerGames(TheHungerGames #...  \n",
       "1  J.K.Rowling MaryGrandPré HarryPotterandtheSorc...  \n",
       "2  StephenieMeyer Twilight(Twilight #1) young-adu...  \n",
       "3  HarperLee ToKillaMockingbird classics classic ...  \n",
       "4  F.ScottFitzgerald TheGreatGatsby classics fict...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assuming df_books_content is your current DataFrame\n",
    "\n",
    "# Function to clean and combine features\n",
    "def clean_and_combine_features(row):\n",
    "    # Ensure authors and title are clean and concatenated\n",
    "    authors = str(row['authors']).replace(' ', '').replace(',', ' ')\n",
    "    title = str(row['title']).replace(' ', '').replace(',', ' ')\n",
    "    \n",
    "    # The tag_soup already has spaces replaced by hyphens, which is good\n",
    "    tags = str(row['tag_soup'])\n",
    "    \n",
    "    # Combine the key features into one string\n",
    "    return authors + ' ' + title + ' ' + tags\n",
    "\n",
    "# Create the new 'content_soup' column\n",
    "df['content_soup'] = df.apply(clean_and_combine_features, axis=1)\n",
    "\n",
    "# print(\"\\nContent Soup Sample (Cleaned and Combined Features):\")\n",
    "# print(df[['title', 'content_soup']].head(2).to_string())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec8ad15",
   "metadata": {},
   "source": [
    "Preparing the ratings.csv data to be used for neural collaborative filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ecca660b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Unique Users: 53424, Total Unique Books: 10000\n",
      "Number of Positive Interactions: 4122111\n",
      "Generating 6183166 candidate samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_207646/476226398.py:76: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_negative['target'] = 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimization Complete.\n",
      "Total final training samples: 8244222\n",
      "Positive samples: 4122111, Negative samples: 4122111\n",
      "Final Training Data Sample:\n",
      "   user_index  book_index  target\n",
      "0       32947        1595       0\n",
      "1       52898        3294       0\n",
      "2       43062         133       0\n",
      "3       42834        3223       1\n",
      "4        7919        9357       0\n"
     ]
    }
   ],
   "source": [
    "df_cf_input = pd.read_csv(\"ratings.csv\")\n",
    "# Assuming df_cf_input is your filtered ratings DataFrame\n",
    "\n",
    "# 1a. Create dense ID mapping\n",
    "user_to_index = {original_id: index for index, original_id in enumerate(df_cf_input['user_id'].unique())}\n",
    "book_to_index = {original_id: index for index, original_id in enumerate(df_cf_input['book_id'].unique())}\n",
    "\n",
    "df_cf_input['user_index'] = df_cf_input['user_id'].map(user_to_index)\n",
    "df_cf_input['book_index'] = df_cf_input['book_id'].map(book_to_index)\n",
    "\n",
    "n_users = len(user_to_index)\n",
    "n_books = len(book_to_index)\n",
    "\n",
    "# 1b. Create binary target (only using high ratings as positive)\n",
    "POSITIVE_RATING_THRESHOLD = 4\n",
    "df_positive = df_cf_input[df_cf_input['rating'] >= POSITIVE_RATING_THRESHOLD].copy()\n",
    "df_positive['target'] = 1\n",
    "\n",
    "print(f\"Total Unique Users: {n_users}, Total Unique Books: {n_books}\")\n",
    "print(f\"Number of Positive Interactions: {len(df_positive)}\")\n",
    "\n",
    "\n",
    "\n",
    "# --- Assumptions ---\n",
    "# df_positive, n_users, n_books are defined\n",
    "# NEGATIVE_SAMPLE_RATIO = 1 (or your desired ratio)\n",
    "\n",
    "# 1. Get the set of all known interactions (CRITICAL for fast lookup)\n",
    "all_interactions = set(zip(df_cf_input['user_index'], df_cf_input['book_index']))\n",
    "positive_samples_count = len(df_positive)\n",
    "\n",
    "# 2. Determine the target number of negative samples\n",
    "NEGATIVE_SAMPLE_RATIO=1\n",
    "target_neg_count = positive_samples_count * NEGATIVE_SAMPLE_RATIO\n",
    "\n",
    "# --- OPTIMIZATION STARTS HERE ---\n",
    "\n",
    "# 3. Generate a large pool of random (user, book) candidates\n",
    "# We generate 1.5x the needed amount to account for overlaps with positive interactions\n",
    "OVERSAMPLE_FACTOR = 1.5\n",
    "total_candidates_to_generate = int(target_neg_count * OVERSAMPLE_FACTOR)\n",
    "\n",
    "print(f\"Generating {total_candidates_to_generate} candidate samples...\")\n",
    "\n",
    "# Vectorized generation of random user and book indices\n",
    "random_user_indices = np.random.randint(0, n_users, total_candidates_to_generate)\n",
    "random_book_indices = np.random.randint(0, n_books, total_candidates_to_generate)\n",
    "\n",
    "# Create a temporary DataFrame of candidate negative samples\n",
    "df_candidates = pd.DataFrame({\n",
    "    'user_index': random_user_indices,\n",
    "    'book_index': random_book_indices,\n",
    "})\n",
    "\n",
    "# 4. Filter out any samples that are already in the 'all_interactions' set\n",
    "\n",
    "# Convert the candidates into tuples for fast set lookup\n",
    "candidate_interactions = set(zip(df_candidates['user_index'], df_candidates['book_index']))\n",
    "\n",
    "# Identify which candidates are actual interactions (collisions)\n",
    "# This is a highly optimized set operation\n",
    "valid_neg_interactions = list(candidate_interactions - all_interactions)\n",
    "\n",
    "# 5. Select the required number of valid negative samples\n",
    "# We must ensure we only take the exact number needed (target_neg_count)\n",
    "df_valid_neg = pd.DataFrame(valid_neg_interactions, columns=['user_index', 'book_index'])\n",
    "\n",
    "# Ensure we have enough and slice to the target count\n",
    "if len(df_valid_neg) < target_neg_count:\n",
    "    print(\"\\nWARNING: Not enough unique negative samples generated. Increase OVERSAMPLE_FACTOR.\")\n",
    "    df_negative = df_valid_neg\n",
    "else:\n",
    "    df_negative = df_valid_neg.head(target_neg_count)\n",
    "\n",
    "# Add the target column\n",
    "df_negative['target'] = 0\n",
    "\n",
    "# --- Final Consolidation ---\n",
    "\n",
    "# 6. Combine positive and negative samples\n",
    "df_training_final = pd.concat([df_positive[['user_index', 'book_index', 'target']], df_negative], ignore_index=True)\n",
    "\n",
    "# 7. Shuffle the data\n",
    "df_training_final = df_training_final.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# 8. Print Results\n",
    "print(f\"\\nOptimization Complete.\")\n",
    "print(f\"Total final training samples: {len(df_training_final)}\")\n",
    "print(f\"Positive samples: {len(df_positive)}, Negative samples: {len(df_negative)}\")\n",
    "print(\"Final Training Data Sample:\")\n",
    "print(df_training_final.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12aad795",
   "metadata": {},
   "source": [
    "Now our nerual collaborative filtering step begins\n",
    "Step 1 is to split our data into training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9aa1c9dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Samples: 6595377\n",
      "Validation Samples: 1648845\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Separate the features (inputs) from the target (output)\n",
    "users = df_training_final['user_index'].values\n",
    "books = df_training_final['book_index'].values\n",
    "targets = df_training_final['target'].values\n",
    "\n",
    "# Split into 80% training and 20% validation sets\n",
    "X_train_u, X_val_u, X_train_i, X_val_i, y_train, y_val = train_test_split(\n",
    "    users, books, targets, test_size=0.2, random_state=42, stratify=targets\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining Samples: {len(X_train_u)}\")\n",
    "print(f\"Validation Samples: {len(X_val_u)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c151aa",
   "metadata": {},
   "source": [
    "Step 2 is defining the base ncf model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d33b3759",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-12 14:47:57.246436: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NCF Model Architecture Summary:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-12 14:48:02.753819: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ user_input          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ item_input          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ user_mlp_embedding  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">534,240</span> │ user_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ item_mlp_embedding  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">100,000</span> │ item_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ user_mlp_embeddi… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ item_mlp_embeddi… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ user_gmf_embedding  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">534,240</span> │ user_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ item_gmf_embedding  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">100,000</span> │ item_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ flatten_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ flatten_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ user_gmf_embeddi… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ item_gmf_embeddi… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">420</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multiply (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Multiply</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ flatten[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    │\n",
       "│                     │                   │            │ flatten_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">210</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multiply[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span> │ concatenate_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ user_input          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ item_input          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ user_mlp_embedding  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m10\u001b[0m)     │    \u001b[38;5;34m534,240\u001b[0m │ user_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ item_mlp_embedding  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m10\u001b[0m)     │    \u001b[38;5;34m100,000\u001b[0m │ item_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten_2 (\u001b[38;5;33mFlatten\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ user_mlp_embeddi… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten_3 (\u001b[38;5;33mFlatten\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ item_mlp_embeddi… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ user_gmf_embedding  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m10\u001b[0m)     │    \u001b[38;5;34m534,240\u001b[0m │ user_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ item_gmf_embedding  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m10\u001b[0m)     │    \u001b[38;5;34m100,000\u001b[0m │ item_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ flatten_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ flatten_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ user_gmf_embeddi… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ item_gmf_embeddi… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)        │        \u001b[38;5;34m420\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multiply (\u001b[38;5;33mMultiply\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ flatten[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],    │\n",
       "│                     │                   │            │ flatten_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)        │        \u001b[38;5;34m210\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ multiply[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ output (\u001b[38;5;33mDense\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │         \u001b[38;5;34m21\u001b[0m │ concatenate_1[\u001b[38;5;34m0\u001b[0m]… │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,269,131</span> (4.84 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,269,131\u001b[0m (4.84 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,269,131</span> (4.84 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,269,131\u001b[0m (4.84 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, Input, Flatten, Concatenate, Dense, Multiply\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Define the model function again (ensure you have n_users and n_books in scope)\n",
    "def build_ncf_model(num_users, num_items, embedding_dim=10):\n",
    "    # --- Input Layers ---\n",
    "    user_input = Input(shape=(1,), name='user_input')\n",
    "    item_input = Input(shape=(1,), name='item_input')\n",
    "\n",
    "    # --- Embedding Layers (Shared for GMF and MLP paths) ---\n",
    "    # GMF Path Embeddings\n",
    "    user_embedding_gmf = Embedding(input_dim=num_users, output_dim=embedding_dim, name='user_gmf_embedding')(user_input)\n",
    "    item_embedding_gmf = Embedding(input_dim=num_items, output_dim=embedding_dim, name='item_gmf_embedding')(item_input)\n",
    "    # MLP Path Embeddings\n",
    "    user_embedding_mlp = Embedding(input_dim=num_users, output_dim=embedding_dim, name='user_mlp_embedding')(user_input)\n",
    "    item_embedding_mlp = Embedding(input_dim=num_items, output_dim=embedding_dim, name='item_mlp_embedding')(item_input)\n",
    "\n",
    "    # --- GMF Path (Linear Interaction) ---\n",
    "    gmf_user_flat = Flatten()(user_embedding_gmf)\n",
    "    gmf_item_flat = Flatten()(item_embedding_gmf)\n",
    "    gmf_interaction = Multiply()([gmf_user_flat, gmf_item_flat])\n",
    "\n",
    "    # --- MLP Path (Non-linear Interaction) ---\n",
    "    mlp_user_flat = Flatten()(user_embedding_mlp)\n",
    "    mlp_item_flat = Flatten()(item_embedding_mlp)\n",
    "    mlp_interaction = Concatenate()([mlp_user_flat, mlp_item_flat])\n",
    "    \n",
    "    # Simple DNN layers for non-linear learning\n",
    "    mlp_layer = Dense(embedding_dim * 2, activation='relu')(mlp_interaction)\n",
    "    mlp_layer = Dense(embedding_dim, activation='relu')(mlp_layer)\n",
    "\n",
    "    # --- Fusion Layer (Combine GMF and MLP) ---\n",
    "    fusion = Concatenate()([gmf_interaction, mlp_layer])\n",
    "    \n",
    "    # --- Prediction Layer ---\n",
    "    # Output uses sigmoid for binary classification (predicting P(interaction))\n",
    "    output = Dense(1, activation='sigmoid', name='output')(fusion)\n",
    "\n",
    "    model = Model(inputs=[user_input, item_input], outputs=output)\n",
    "    return model\n",
    "\n",
    "# Initialize and compile the model\n",
    "EMBEDDING_DIM = 10 # Hyperparameter: determines the size of the latent factors\n",
    "ncf_model = build_ncf_model(n_users, n_books, embedding_dim=EMBEDDING_DIM)\n",
    "ncf_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), \n",
    "                  loss='binary_crossentropy') # Binary Cross-Entropy for binary classification (0 or 1)\n",
    "\n",
    "print(\"\\nNCF Model Architecture Summary:\")\n",
    "ncf_model.summary()\n",
    "\n",
    "# Image of Neural Collaborative Filtering Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499742e7",
   "metadata": {},
   "source": [
    "Step 3 is training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e7a71a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "\u001b[1m   39/25764\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10:51\u001b[0m 25ms/step - loss: 0.6932"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m BATCH_SIZE = \u001b[32m256\u001b[39m\n\u001b[32m      3\u001b[39m EPOCHS = \u001b[32m3\u001b[39m \u001b[38;5;66;03m# Start with 10 epochs, adjust based on validation loss\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m history = \u001b[43mncf_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mX_train_u\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train_i\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mX_val_u\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val_i\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mModel Training Complete!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/.venv/lib/python3.13/site-packages/keras/src/utils/traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/.venv/lib/python3.13/site-packages/keras/src/backend/tensorflow/trainer.py:399\u001b[39m, in \u001b[36mTensorFlowTrainer.fit\u001b[39m\u001b[34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[39m\n\u001b[32m    397\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m begin_step, end_step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[32m    398\u001b[39m     callbacks.on_train_batch_begin(begin_step)\n\u001b[32m--> \u001b[39m\u001b[32m399\u001b[39m     logs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    400\u001b[39m     callbacks.on_train_batch_end(end_step, logs)\n\u001b[32m    401\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stop_training:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/.venv/lib/python3.13/site-packages/keras/src/backend/tensorflow/trainer.py:241\u001b[39m, in \u001b[36mTensorFlowTrainer._make_function.<locals>.function\u001b[39m\u001b[34m(iterator)\u001b[39m\n\u001b[32m    237\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfunction\u001b[39m(iterator):\n\u001b[32m    238\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[32m    239\u001b[39m         iterator, (tf.data.Iterator, tf.distribute.DistributedIterator)\n\u001b[32m    240\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m241\u001b[39m         opt_outputs = \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    242\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs.has_value():\n\u001b[32m    243\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/.venv/lib/python3.13/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/.venv/lib/python3.13/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[39m, in \u001b[36mFunction.__call__\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    830\u001b[39m compiler = \u001b[33m\"\u001b[39m\u001b[33mxla\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mnonXla\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m._jit_compile):\n\u001b[32m--> \u001b[39m\u001b[32m833\u001b[39m   result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    835\u001b[39m new_tracing_count = \u001b[38;5;28mself\u001b[39m.experimental_get_tracing_count()\n\u001b[32m    836\u001b[39m without_tracing = (tracing_count == new_tracing_count)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/.venv/lib/python3.13/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[39m, in \u001b[36mFunction._call\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    875\u001b[39m \u001b[38;5;28mself\u001b[39m._lock.release()\n\u001b[32m    876\u001b[39m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[32m    877\u001b[39m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m878\u001b[39m results = \u001b[43mtracing_compilation\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    881\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._created_variables:\n\u001b[32m    882\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCreating variables on a non-first call to a function\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    883\u001b[39m                    \u001b[33m\"\u001b[39m\u001b[33m decorated with tf.function.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/.venv/lib/python3.13/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[39m, in \u001b[36mcall_function\u001b[39m\u001b[34m(args, kwargs, tracing_options)\u001b[39m\n\u001b[32m    137\u001b[39m bound_args = function.function_type.bind(*args, **kwargs)\n\u001b[32m    138\u001b[39m flat_inputs = function.function_type.unpack_inputs(bound_args)\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[32m    140\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/.venv/lib/python3.13/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[39m, in \u001b[36mConcreteFunction._call_flat\u001b[39m\u001b[34m(self, tensor_inputs, captured_inputs)\u001b[39m\n\u001b[32m   1318\u001b[39m possible_gradient_type = gradients_util.PossibleTapeGradientTypes(args)\n\u001b[32m   1319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type == gradients_util.POSSIBLE_GRADIENT_TYPES_NONE\n\u001b[32m   1320\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[32m   1321\u001b[39m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inference_function\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1323\u001b[39m forward_backward = \u001b[38;5;28mself\u001b[39m._select_forward_and_backward_functions(\n\u001b[32m   1324\u001b[39m     args,\n\u001b[32m   1325\u001b[39m     possible_gradient_type,\n\u001b[32m   1326\u001b[39m     executing_eagerly)\n\u001b[32m   1327\u001b[39m forward_function, args_with_tangents = forward_backward.forward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/.venv/lib/python3.13/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[39m, in \u001b[36mAtomicFunction.call_preflattened\u001b[39m\u001b[34m(self, args)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core.Tensor]) -> Any:\n\u001b[32m    215\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m   flat_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.function_type.pack_output(flat_outputs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/.venv/lib/python3.13/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[39m, in \u001b[36mAtomicFunction.call_flat\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m record.stop_recording():\n\u001b[32m    250\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._bound_context.executing_eagerly():\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_bound_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction_type\u001b[49m\u001b[43m.\u001b[49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    256\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    257\u001b[39m     outputs = make_call_op_in_graph(\n\u001b[32m    258\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    259\u001b[39m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[32m    260\u001b[39m         \u001b[38;5;28mself\u001b[39m._bound_context.function_call_options.as_attrs(),\n\u001b[32m    261\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/.venv/lib/python3.13/site-packages/tensorflow/python/eager/context.py:1688\u001b[39m, in \u001b[36mContext.call_function\u001b[39m\u001b[34m(self, name, tensor_inputs, num_outputs)\u001b[39m\n\u001b[32m   1686\u001b[39m cancellation_context = cancellation.context()\n\u001b[32m   1687\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1688\u001b[39m   outputs = \u001b[43mexecute\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1689\u001b[39m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1690\u001b[39m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1691\u001b[39m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1692\u001b[39m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1693\u001b[39m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1694\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1695\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1696\u001b[39m   outputs = execute.execute_with_cancellation(\n\u001b[32m   1697\u001b[39m       name.decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   1698\u001b[39m       num_outputs=num_outputs,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1702\u001b[39m       cancellation_manager=cancellation_context,\n\u001b[32m   1703\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/.venv/lib/python3.13/site-packages/tensorflow/python/eager/execute.py:53\u001b[39m, in \u001b[36mquick_execute\u001b[39m\u001b[34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     52\u001b[39m   ctx.ensure_initialized()\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m   tensors = \u001b[43mpywrap_tfe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     56\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS = 3 # Start with 10 epochs, adjust based on validation loss\n",
    "\n",
    "history = ncf_model.fit(\n",
    "    [X_train_u, X_train_i], y_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    verbose=1,\n",
    "    validation_data=([X_val_u, X_val_i], y_val)\n",
    ")\n",
    "\n",
    "print(\"\\nModel Training Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0b568f",
   "metadata": {},
   "source": [
    "now predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "96965449",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def recommend_ncf_for_user(ncf_model, user_id, df_cf_input, book_to_index, n_books, top_k=10):\n",
    "    \n",
    "    # 1. Map the original user_id to the internal index\n",
    "    user_index = user_to_index.get(user_id)\n",
    "    \n",
    "    if user_index is None:\n",
    "        print(f\"Error: User ID {user_id} not found in the trained dataset.\")\n",
    "        return []\n",
    "\n",
    "    # 2. Identify books the user HAS interacted with (exclude these)\n",
    "    user_interactions = df_cf_input[df_cf_input['user_id'] == user_id]['book_index'].unique()\n",
    "    \n",
    "    # 3. Identify all book indices the user HAS NOT interacted with (candidates)\n",
    "    all_book_indices = np.arange(n_books)\n",
    "    candidate_book_indices = np.setdiff1d(all_book_indices, user_interactions)\n",
    "    \n",
    "    # 4. Prepare input for the model\n",
    "    # The model expects two arrays: (user_indices, book_indices)\n",
    "    user_indices_array = np.full(len(candidate_book_indices), user_index)\n",
    "    \n",
    "    # 5. Predict scores (inference)\n",
    "    predictions = ncf_model.predict([user_indices_array, candidate_book_indices], verbose=0).flatten()\n",
    "    \n",
    "    # 6. Rank the candidates\n",
    "    # Get the indices that would sort the predictions in descending order\n",
    "    top_k_indices = predictions.argsort()[-top_k:][::-1]\n",
    "    \n",
    "    # Get the corresponding book indices (the *internal* IDs)\n",
    "    top_book_indices = candidate_book_indices[top_k_indices]\n",
    "    top_scores = predictions[top_k_indices]\n",
    "    \n",
    "    # 7. Map back to original book_id (the key to your main books table)\n",
    "    \n",
    "    # Create the inverse map (index_to_book)\n",
    "    index_to_book = {v: k for k, v in book_to_index.items()}\n",
    "    \n",
    "    # Map the internal index back to the external book_id\n",
    "    top_book_ids = [index_to_book[idx] for idx in top_book_indices]\n",
    "\n",
    "    # 8. Return results\n",
    "    results = pd.DataFrame({\n",
    "        'original_book_id': top_book_ids,\n",
    "        'ncf_score': top_scores\n",
    "    })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example Usage: Assuming you want recommendations for a reliable user with original user_id=2\n",
    "# Replace 2 with a real user_id from your dataset\n",
    "# user_id_to_test = 2 \n",
    "# recommendations = recommend_ncf_for_user(ncf_model, user_id_to_test, df_cf_input, book_to_index, n_books)\n",
    "# print(f\"\\nRecommendations for User {user_id_to_test}:\\n\", recommendations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78e7667",
   "metadata": {},
   "source": [
    "using the model to get for some user id, the top k books it would highly rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f94cb420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating NCF recommendations for User ID: 1\n",
      "\n",
      "--- Raw NCF Output (Original Book IDs and Scores) ---\n",
      "   original_book_id  ncf_score\n",
      "0               158   0.512003\n",
      "1                18   0.510697\n",
      "2                 8   0.509005\n",
      "3              4305   0.508754\n",
      "4               513   0.508528\n",
      "5               749   0.508216\n",
      "6              4856   0.508060\n",
      "7              3102   0.507970\n",
      "8              6516   0.507941\n",
      "9               117   0.507905\n",
      "\n",
      "--- Final Personalized Recommendations (NCF) ---\n",
      "   Predicted Score (NCF)                                                           title                                             authors  average_rating\n",
      "0               0.512003          Charlie and the Chocolate Factory (Charlie Bucket, #1)                           Roald Dahl, Quentin Blake            4.10\n",
      "1               0.510697     Harry Potter and the Prisoner of Azkaban (Harry Potter, #3)             J.K. Rowling, Mary GrandPré, Rufus Beck            4.53\n",
      "2               0.509005                                          The Catcher in the Rye                                       J.D. Salinger            3.79\n",
      "3               0.508754                               Night Broken (Mercy Thompson, #8)                                     Patricia Briggs            4.42\n",
      "4               0.508528  The Hiding Place: The Triumphant True Story of Corrie Ten Boom  Corrie ten Boom, John Sherrill, Elizabeth Sherrill            4.42\n",
      "5               0.508216                                                 The Storyteller                                        Jodi Picoult            4.26\n",
      "6               0.508060                                                   TransAtlantic                                        Colum McCann            3.81\n",
      "7               0.507970                                           The Weight of Silence                                   Heather Gudenkauf            3.93\n",
      "8               0.507941                           The Dogs of Riga (Kurt Wallander, #2)                    Henning Mankell, Laurie Thompson            3.72\n",
      "9               0.507905               A Wrinkle in Time (A Wrinkle in Time Quintet, #1)                                   Madeleine L'Engle            4.04\n"
     ]
    }
   ],
   "source": [
    "# --- Find a Valid User ID to Test ---\n",
    "# Get the first user_id from the set of users the model was trained on\n",
    "user_id_to_test = df_cf_input['user_id'].iloc[0]\n",
    "\n",
    "print(f\"Generating NCF recommendations for User ID: {user_id_to_test}\\n\")\n",
    "\n",
    "# Call your defined recommendation function\n",
    "recommendations_ncf = recommend_ncf_for_user(\n",
    "    ncf_model=ncf_model,\n",
    "    user_id=user_id_to_test,\n",
    "    df_cf_input=df_cf_input,\n",
    "    book_to_index=book_to_index,\n",
    "    n_books=n_books,\n",
    "    top_k=10\n",
    ")\n",
    "\n",
    "print(\"--- Raw NCF Output (Original Book IDs and Scores) ---\")\n",
    "print(recommendations_ncf)\n",
    "\n",
    "# Assuming your main book content table is named df\n",
    "# Ensure you have loaded or kept df from earlier steps!\n",
    "\n",
    "# Perform a merge to join the predicted scores with the book details\n",
    "final_recommendations_ncf = pd.merge(\n",
    "    recommendations_ncf,\n",
    "    df[['book_id', 'title', 'authors', 'average_rating']], # Select key columns\n",
    "    left_on='original_book_id',\n",
    "    right_on='book_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Clean up and display the final result\n",
    "final_recommendations_ncf = final_recommendations_ncf.sort_values(\n",
    "    'ncf_score', \n",
    "    ascending=False\n",
    ").reset_index(drop=True)\n",
    "\n",
    "# Rename for clarity\n",
    "final_recommendations_ncf = final_recommendations_ncf.rename(\n",
    "    columns={'ncf_score': 'Predicted Score (NCF)'}\n",
    ")\n",
    "\n",
    "print(\"\\n--- Final Personalized Recommendations (NCF) ---\")\n",
    "print(final_recommendations_ncf[[\n",
    "    'Predicted Score (NCF)', \n",
    "    'title', \n",
    "    'authors', \n",
    "    'average_rating'\n",
    "]].head(10).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ed6f13",
   "metadata": {},
   "source": [
    "# Content-Based Filtering (CBF) Model Implementation\n",
    "\n",
    "The Content-Based Filtering component solves the critical **New User/New Item Cold Start Problem** by generating recommendations based purely on descriptive features of the books. This ensures that even users with no prior rating history receive relevant suggestions instantly, and new books (with no ratings) can still be recommended.\n",
    "\n",
    "## 🎯 Core Mechanism: Item Similarity\n",
    "\n",
    "Instead of relying on user behavior, the CBF model generates **book-to-book similarity scores** using their shared content.\n",
    "\n",
    "### 1. Feature Consolidation (`tag_soup`)\n",
    "Features like **title, authors, and descriptive tags** were aggregated into a single text block (`content_soup` or `tag_soup`) for each book.\n",
    "\n",
    "### 2. Vectorization (TF-IDF)\n",
    "The text features were converted into a **sparse numerical matrix** using the **Term Frequency-Inverse Document Frequency (TF-IDF)** scheme.  \n",
    "- TF-IDF weights **rare, distinguishing tags** (e.g., 'Austen', 'dystopian') higher than **common terms** (e.g., 'book', 'fiction').\n",
    "\n",
    "### 3. Similarity Matrix Generation\n",
    "The final step involves calculating the **Cosine Similarity** between every book's TF-IDF vector:\n",
    "\n",
    "\\[\n",
    "\\text{Similarity}(B_i, B_j) = \\text{CosineSimilarity}(\\text{Vector}_{B_i}, \\text{Vector}_{B_j})\n",
    "\\]\n",
    "\n",
    "The result is a comprehensive **Item Similarity Matrix**, where we can instantly look up the content similarity score between any two books in the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ab54327c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting TF-IDF Vectorization...\n",
      "TF-IDF Matrix Shape: (10000, 1164)\n",
      "Calculating Cosine Similarity Matrix (This may take a moment)...\n",
      "Cosine Similarity Matrix Calculation Complete.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "\n",
    "# Assume df is loaded and preprocessed with the 'tag_soup' column\n",
    "\n",
    "print(\"Starting TF-IDF Vectorization...\")\n",
    "\n",
    "# 1. Initialize the TF-IDF Vectorizer\n",
    "# max_df=0.9: Ignore terms that appear in more than 90% of documents (too common)\n",
    "# min_df=5: Ignore terms that appear in fewer than 5 documents (too rare)\n",
    "tfidf = TfidfVectorizer(stop_words='english', max_df=0.9, min_df=5)\n",
    "\n",
    "# Fit the vectorizer to the 'tag_soup' and transform the text into a sparse matrix\n",
    "tfidf_matrix = tfidf.fit_transform(df['tag_soup'])\n",
    "\n",
    "print(f\"TF-IDF Matrix Shape: {tfidf_matrix.shape}\")\n",
    "print(\"Calculating Cosine Similarity Matrix (This may take a moment)...\")\n",
    "\n",
    "# 2. Compute the Cosine Similarity Matrix\n",
    "# The resulting matrix (N x N) holds the similarity score between every pair of books.\n",
    "# This matrix will be the core of your CBF model.\n",
    "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "print(\"Cosine Similarity Matrix Calculation Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6ffa6a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping series to quickly lookup the internal index from the book_id\n",
    "book_to_index_series = pd.Series(df.index, index=df['book_id']).drop_duplicates()\n",
    "\n",
    "def recommend_cbf_by_book_id(book_id, cosine_sim_matrix, df_content, top_k=10):\n",
    "    \n",
    "    # 1. Get the internal index corresponding to the input book_id\n",
    "    try:\n",
    "        idx = book_to_index_series[book_id]\n",
    "    except KeyError:\n",
    "        return f\"Error: Book ID {book_id} not found in content data.\"\n",
    "\n",
    "    # 2. Get the similarity scores for this book against all other books\n",
    "    # The scores are the row/column corresponding to 'idx' in the matrix\n",
    "    sim_scores = list(enumerate(cosine_sim_matrix[idx]))\n",
    "\n",
    "    # 3. Sort the books based on the similarity scores (descending)\n",
    "    # We ignore the first element (index 0) because it's the book itself (similarity = 1.0)\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    sim_scores = sim_scores[1:top_k + 1] \n",
    "\n",
    "    # 4. Get the book indices and similarity scores\n",
    "    book_indices = [i[0] for i in sim_scores]\n",
    "    scores = [i[1] for i in sim_scores]\n",
    "    \n",
    "    # 5. Get the original book IDs and titles for the top recommendations\n",
    "    recommended_books = df_content.iloc[book_indices].copy()\n",
    "    \n",
    "    # 6. Format and return\n",
    "    results = pd.DataFrame({\n",
    "        'original_book_id': recommended_books['book_id'],\n",
    "        'cbf_score': scores,\n",
    "        'title': recommended_books['title'],\n",
    "        'authors': recommended_books['authors']\n",
    "    })\n",
    "    \n",
    "    return results.reset_index(drop=True)\n",
    "\n",
    "# Example Usage: Find books similar to the book with original book_id=1\n",
    "# You need to ensure book_id=1 is in your df\n",
    "# recommendation_cbf = recommend_cbf_by_book_id(1, cosine_sim, df)\n",
    "# print(\"\\n--- Content-Based Recommendations for Book ID 1 ---\")\n",
    "# print(recommendation_cbf.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f56963d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating Content-Based Recommendations ---\n",
      "Anchor Book ID: 71\n",
      "Anchor Book Title: Frankenstein\n",
      "\n",
      "Results based on shared tags/authors (Content-Based Score):\n",
      "----------------------------------------------------------\n",
      "   cbf_score                                                                  title                                                            authors\n",
      "0   0.710223                                             The Picture of Dorian Gray                                     Oscar Wilde, Jeffrey Eugenides\n",
      "1   0.698998                            The Strange Case of Dr. Jekyll and Mr. Hyde  Robert Louis Stevenson, Vladimir Nabokov, Mervyn Peake, Dan Chaon\n",
      "2   0.624072                                                                Dracula                          Bram Stoker, Nina Auerbach, David J. Skal\n",
      "3   0.622619                                     Necronomicon: The Best Weird Tales                         H.P. Lovecraft, Les Edwards, Stephen Jones\n",
      "4   0.606474                                               The Island of Dr. Moreau                                                         H.G. Wells\n",
      "5   0.578673  The Strange Case of Dr. Jekyll and Mr. Hyde and Other Tales of Terror                             Robert Louis Stevenson, Robert Mighall\n",
      "6   0.554458                            The Legend of Sleepy Hollow (Graphic Novel)                      Bo Hampton, Tracey Hampton, Washington Irving\n",
      "7   0.546231                                                              The Pearl                                                     John Steinbeck\n",
      "8   0.544971                                                      The Invisible Man                                                         H.G. Wells\n",
      "9   0.542899                                                              The Other                                                       Thomas Tryon\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Assuming df, cosine_sim, and the function recommend_cbf_by_book_id are defined\n",
    "\n",
    "# --- 1. Select a Book ID to Use as the Anchor ---\n",
    "# We will use the first book ID found in your content data as a reliable test case.\n",
    "book_id_to_test = df['book_id'].iloc[70]\n",
    "\n",
    "# --- 2. Retrieve the Title of the Anchor Book (For context in the output) ---\n",
    "anchor_title = df[df['book_id'] == book_id_to_test]['title'].iloc[0]\n",
    "\n",
    "print(f\"--- Generating Content-Based Recommendations ---\")\n",
    "print(f\"Anchor Book ID: {book_id_to_test}\")\n",
    "print(f\"Anchor Book Title: {anchor_title}\\n\")\n",
    "\n",
    "# --- 3. Run the Recommendation Function ---\n",
    "recommendation_cbf = recommend_cbf_by_book_id(\n",
    "    book_id=book_id_to_test,\n",
    "    cosine_sim_matrix=cosine_sim,\n",
    "    df_content=df,\n",
    "    top_k=10 # Get the top 10 similar books\n",
    ")\n",
    "\n",
    "# --- 4. Display Results ---\n",
    "print(\"Results based on shared tags/authors (Content-Based Score):\")\n",
    "print(\"----------------------------------------------------------\")\n",
    "# Use to_string() for clean console display\n",
    "print(recommendation_cbf[['cbf_score', 'title', 'authors']].to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebe065a",
   "metadata": {},
   "source": [
    "The key challenge here is that the NCF function gives scores for all unseen items, while the CBF function only gives item-to-item similarity scores based on a single anchor book.\n",
    "\n",
    "For a true hybrid, we need to adapt the CBF logic to a User-Centric CBF Score:\n",
    "\n",
    "    Identify User's Favorite Books: Find the books the user rated highest.\n",
    "\n",
    "    Calculate Average Similarity: For every candidate book, calculate its average Content-Based Similarity to the user's favorite books. This gives us a Scbf​ score for that user/item pair.\n",
    "\n",
    "    Fuse Scores: Combine Sncf​ and Scbf​.\n",
    "\n",
    "1. Adaptation: User-Centric CBF Score (Required Pre-Calculation)\n",
    "\n",
    "First, we need a function that calculates the user-centric CBF score for all candidate books."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b2d93ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_user_cbf_scores(user_id, df_cf_input, df_positive, book_to_index_series, cosine_sim_matrix, candidate_book_indices):\n",
    "    \n",
    "    # 1. Identify the user's high-rated books (using internal row indices)\n",
    "    # We use df_positive (filtered high ratings) for a stronger signal\n",
    "    user_pos_ratings = df_positive[df_positive['user_id'] == user_id]\n",
    "    \n",
    "    # Get the row indices from df_books_content for these positive books\n",
    "    # Note: We are using the original book_id here to map to the content DataFrame index\n",
    "    user_fav_book_ids = user_pos_ratings['book_id'].unique()\n",
    "    \n",
    "    # Filter out any favorites that might not be in the content DataFrame\n",
    "    valid_fav_indices = [book_to_index_series[bid] for bid in user_fav_book_ids if bid in book_to_index_series]\n",
    "    \n",
    "    if not valid_fav_indices:\n",
    "        return pd.Series(0.0, index=candidate_book_indices) # Return 0 scores if user has no favorites\n",
    "\n",
    "    # 2. Calculate the average content similarity for all candidates\n",
    "    \n",
    "    # Extract the similarity scores for all candidate books across the rows of the user's favorites\n",
    "    # This gives us a (Num_Favorites x Num_Candidates) matrix\n",
    "    sims_to_favorites = cosine_sim_matrix[valid_fav_indices][:, candidate_book_indices]\n",
    "    \n",
    "    # Average the scores across the favorites axis (axis=0) to get one score per candidate book\n",
    "    avg_cbf_scores = np.mean(sims_to_favorites, axis=0)\n",
    "    \n",
    "    # Return as a pandas Series for easy merging later (indexed by candidate book index)\n",
    "    return pd.Series(avg_cbf_scores, index=candidate_book_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24fd2f4",
   "metadata": {},
   "source": [
    "2. The Final Hybrid Function\n",
    "\n",
    "This function puts the NCF score, the new CBF score, and the weighted fusion together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "03b6ee17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_hybrid(user_id, ncf_model, df_cf_input, df_positive, book_to_index, book_to_index_series, n_books, cosine_sim_matrix, top_k=10, w_ncf=0.7, w_cbf=0.3):\n",
    "    \n",
    "    # --- Part A: Get NCF Scores and Candidate List ---\n",
    "    \n",
    "    # 1. Run the NCF prediction function (we adapt the logic slightly for efficiency)\n",
    "    # Re-use the logic from recommend_ncf_for_user to get the base predictions and candidates\n",
    "    \n",
    "    # Get user index\n",
    "    user_index = user_to_index.get(user_id)\n",
    "    if user_index is None:\n",
    "        return \"User not found.\"\n",
    "\n",
    "    # Identify candidates\n",
    "    user_interactions = df_cf_input[df_cf_input['user_id'] == user_id]['book_index'].unique()\n",
    "    all_book_indices = np.arange(n_books)\n",
    "    candidate_book_indices = np.setdiff1d(all_book_indices, user_interactions)\n",
    "    \n",
    "    # Run NCF prediction\n",
    "    user_indices_array = np.full(len(candidate_book_indices), user_index)\n",
    "    ncf_scores_array = ncf_model.predict([user_indices_array, candidate_book_indices], verbose=0).flatten()\n",
    "    \n",
    "    # Create base DataFrame\n",
    "    df_scores = pd.DataFrame({\n",
    "        'book_index': candidate_book_indices,\n",
    "        'ncf_score': ncf_scores_array\n",
    "    }).set_index('book_index')\n",
    "\n",
    "    \n",
    "    # --- Part B: Get CBF Scores ---\n",
    "\n",
    "    # 2. Calculate the User-Centric CBF Score for all candidates\n",
    "    cbf_scores_series = calculate_user_cbf_scores(\n",
    "        user_id, df_cf_input, df_positive, book_to_index_series, cosine_sim_matrix, candidate_book_indices\n",
    "    )\n",
    "    \n",
    "    # Merge CBF scores onto the main score DataFrame\n",
    "    df_scores['cbf_score'] = cbf_scores_series\n",
    "    \n",
    "    \n",
    "    # --- Part C: Fusion and Final Ranking ---\n",
    "    \n",
    "    # 3. Apply the Hybrid Fusion Formula\n",
    "    df_scores['hybrid_score'] = (w_ncf * df_scores['ncf_score']) + (w_cbf * df_scores['cbf_score'])\n",
    "    \n",
    "    # 4. Final Ranking\n",
    "    top_recommendations = df_scores.sort_values('hybrid_score', ascending=False).head(top_k)\n",
    "    \n",
    "    \n",
    "    # --- Part D: Mapping and Formatting ---\n",
    "    \n",
    "    # Create the inverse map (index_to_book)\n",
    "    index_to_book = {v: k for k, v in book_to_index.items()}\n",
    "    \n",
    "    # Map the internal index back to the external book_id\n",
    "    top_recommendations['original_book_id'] = [index_to_book[idx] for idx in top_recommendations.index]\n",
    "    \n",
    "    return top_recommendations.reset_index().drop(columns=['book_index'])\n",
    "\n",
    "\n",
    "# --- Example Usage (Requires all variables to be defined) ---\n",
    "\n",
    "# Find a valid user to test (e.g., the first user in the positive ratings)\n",
    "# user_id_to_test = df_positive['user_id'].iloc[0]\n",
    "\n",
    "# final_recs = recommend_hybrid(\n",
    "#     user_id=user_id_to_test,\n",
    "#     ncf_model=ncf_model,\n",
    "#     df_cf_input=df_cf_input,\n",
    "#     df_positive=df_positive,\n",
    "#     book_to_index=book_to_index,\n",
    "#     book_to_index_series=book_to_index_series,\n",
    "#     n_books=n_books,\n",
    "#     cosine_sim_matrix=cosine_sim,\n",
    "#     w_ncf=0.7, # 70% weight to personalization\n",
    "#     w_cbf=0.3  # 30% weight to relevance/content\n",
    "# )\n",
    "\n",
    "# print(f\"\\n--- Final Hybrid Recommendations for User {user_id_to_test} ---\")\n",
    "# print(final_recs)\n",
    "\n",
    "# To display titles, you would merge final_recs with df_books_content on 'original_book_id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fbca4d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating HYBRID Recommendations for User ID: 1 ---\n",
      "Fusion Weights: NCF=0.7, CBF=0.3\n",
      "\n",
      "Final Hybrid Score Breakdown (Top 10):\n",
      "--------------------------------------\n",
      " Hybrid Score  NCF (Taste)  CBF (Content)                                                                           Book Title                                            Authors  Avg Rating\n",
      "     0.445813     0.502352       0.313888                                                                                Proof                                       David Auburn        4.01\n",
      "     0.441307     0.500921       0.302209                The Millionaire Next Door: The Surprising Secrets of Americas Wealthy                Thomas J. Stanley, William D. Danko        4.00\n",
      "     0.440193     0.503128       0.293345                                                                          The Prophet                                      Kahlil Gibran        4.22\n",
      "     0.439903     0.501527       0.296115                                                                     Love You Forever                       Robert Munsch, Sheila McGraw        4.34\n",
      "     0.438953     0.499804       0.296967                                                      Slapstick, or Lonesome No More!                                  Kurt Vonnegut Jr.        3.87\n",
      "     0.436638     0.500536       0.287543                                                            The Complete Short Novels Anton Chekhov, Richard Pevear, Larissa Volokhonsky        4.47\n",
      "     0.435849     0.503069       0.279002 Crossing the Chasm: Marketing and Selling High-Tech Products to Mainstream Customers                   Geoffrey A. Moore, Regis McKenna        4.00\n",
      "     0.435549     0.500649       0.283648                                                        Be with Me (Wait for You, #2)                    J. Lynn, Jennifer L. Armentrout        4.15\n",
      "     0.435234     0.501971       0.279513                                                                     Chasing the Dime                                   Michael Connelly        3.96\n",
      "     0.435213     0.502909       0.277254                                                                  The Prince of Tides                                         Pat Conroy        4.22\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Assumes: ncf_model, df_cf_input, df_positive, book_to_index, book_to_index_series, \n",
    "# n_books, cosine_sim_matrix, and df_books_content are all defined.\n",
    "# Assumes: The recommend_hybrid and calculate_user_cbf_scores functions are defined.\n",
    "\n",
    "# --- 1. Find a Valid User ID to Test ---\n",
    "# We select a user who has positive ratings, ensuring both NCF and CBF signals are active.\n",
    "user_id_to_test = df_positive['user_id'].iloc[0] \n",
    "\n",
    "# --- 2. Define Fusion Weights ---\n",
    "# These are the hyperparameters that control the balance of the hybrid system.\n",
    "W_NCF = 0.7  # Weight for personalized taste (NCF)\n",
    "W_CBF = 0.3  # Weight for content relevance (CBF/TF-IDF)\n",
    "\n",
    "print(f\"--- Generating HYBRID Recommendations for User ID: {user_id_to_test} ---\")\n",
    "print(f\"Fusion Weights: NCF={W_NCF}, CBF={W_CBF}\\n\")\n",
    "\n",
    "# --- 3. Execute the Hybrid Recommendation Function ---\n",
    "final_recs_raw = recommend_hybrid(\n",
    "    user_id=user_id_to_test,\n",
    "    ncf_model=ncf_model,\n",
    "    df_cf_input=df_cf_input,\n",
    "    df_positive=df_positive,\n",
    "    book_to_index=book_to_index,\n",
    "    book_to_index_series=book_to_index_series,\n",
    "    n_books=n_books,\n",
    "    cosine_sim_matrix=cosine_sim,\n",
    "    top_k=10, \n",
    "    w_ncf=W_NCF, \n",
    "    w_cbf=W_CBF\n",
    ")\n",
    "\n",
    "# --- 4. Merge Raw Scores with Book Content for Readability ---\n",
    "# Join the raw results (which contain the calculated scores) with the main content DataFrame\n",
    "final_recs_display = pd.merge(\n",
    "    final_recs_raw,\n",
    "    df[['book_id', 'title', 'authors', 'average_rating']],\n",
    "    left_on='original_book_id',\n",
    "    right_on='book_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# --- 5. Display Final Results ---\n",
    "\n",
    "print(\"Final Hybrid Score Breakdown (Top 10):\")\n",
    "print(\"--------------------------------------\")\n",
    "\n",
    "# Select and rename columns for a clean output table\n",
    "output_columns = {\n",
    "    'hybrid_score': 'Hybrid Score',\n",
    "    'ncf_score': 'NCF (Taste)',\n",
    "    'cbf_score': 'CBF (Content)',\n",
    "    'title': 'Book Title',\n",
    "    'authors': 'Authors',\n",
    "    'average_rating': 'Avg Rating'\n",
    "}\n",
    "\n",
    "final_recs_display = final_recs_display.rename(columns=output_columns)\n",
    "\n",
    "# Display the final, beautifully organized table\n",
    "print(final_recs_display[[\n",
    "    'Hybrid Score',\n",
    "    'NCF (Taste)',\n",
    "    'CBF (Content)',\n",
    "    'Book Title',\n",
    "    'Authors',\n",
    "    'Avg Rating'\n",
    "]].to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
