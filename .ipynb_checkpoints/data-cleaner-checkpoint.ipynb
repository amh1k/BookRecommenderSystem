{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbb2fbb2",
   "metadata": {},
   "source": [
    "Importing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54a9cfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57589ba3",
   "metadata": {},
   "source": [
    "trying to get the shape of data and removing unncessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1c8b9cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "book_id                        int64\n",
       "goodreads_book_id              int64\n",
       "authors                       object\n",
       "original_publication_year    float64\n",
       "original_title                object\n",
       "title                         object\n",
       "average_rating               float64\n",
       "ratings_count                  int64\n",
       "work_ratings_count             int64\n",
       "work_text_reviews_count        int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"books.csv\")\n",
    "df_book_tags = pd.read_csv(\"book_tags.csv\")\n",
    "df_tags = pd.read_csv(\"tags.csv\")\n",
    "# \n",
    "df = df.drop([\n",
    "    'work_id', 'isbn', 'isbn13', 'language_code', 'image_url', 'small_image_url', 'best_book_id', 'books_count','ratings_1', 'ratings_2', 'ratings_3', 'ratings_4', 'ratings_5'\n",
    "],axis = 1)\n",
    "df.dtypes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccf732c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>goodreads_book_id</th>\n",
       "      <th>tag_id</th>\n",
       "      <th>count</th>\n",
       "      <th>tag_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>30574</td>\n",
       "      <td>167697</td>\n",
       "      <td>to-read</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>11305</td>\n",
       "      <td>37174</td>\n",
       "      <td>fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>11557</td>\n",
       "      <td>34173</td>\n",
       "      <td>favorites</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>8717</td>\n",
       "      <td>12986</td>\n",
       "      <td>currently-reading</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>33114</td>\n",
       "      <td>12716</td>\n",
       "      <td>young-adult</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   goodreads_book_id  tag_id   count           tag_name\n",
       "0                  1   30574  167697            to-read\n",
       "1                  1   11305   37174            fantasy\n",
       "2                  1   11557   34173          favorites\n",
       "3                  1    8717   12986  currently-reading\n",
       "4                  1   33114   12716        young-adult"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tags_named = pd.merge(\n",
    "    df_book_tags,\n",
    "    df_tags,\n",
    "    on='tag_id',\n",
    "    how='left' # Keep all tag records, even if a tag_name is missing (rare)\n",
    ")\n",
    "df_tags_named.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7c86f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated Tags Sample:\n",
      "   goodreads_book_id                                           tag_soup\n",
      "0                  1  fantasy young-adult fiction harry-potter books...\n",
      "1                  2  fantasy children children-s all-time-favorites...\n",
      "2                  3  fantasy young-adult fiction harry-potter books...\n",
      "3                  5  fantasy young-adult fiction harry-potter books...\n",
      "4                  6  fantasy young-adult fiction harry-potter ya se...\n"
     ]
    }
   ],
   "source": [
    "# List of generic tags to exclude (these describe user intent, not book content)\n",
    "GENERIC_TAGS_TO_REMOVE = [\n",
    "    'to-read',\n",
    "    'currently-reading',\n",
    "    'owned',\n",
    "    'favorites',\n",
    "    'default',\n",
    "    'ebook',\n",
    "    'kindle',\n",
    "    'library',\n",
    "    'owned-books',\n",
    "    'my-books',\n",
    "    'books'\n",
    "]\n",
    "\n",
    "# 1. Filter out generic tags\n",
    "df_content_tags = df_tags_named[~df_tags_named['tag_name'].isin(GENERIC_TAGS_TO_REMOVE)].copy()\n",
    "\n",
    "# Sort by count (descending) to prioritize the most important tags for each book\n",
    "df_content_tags = df_content_tags.sort_values(\n",
    "    ['goodreads_book_id', 'count'],\n",
    "    ascending=[True, False]\n",
    ")\n",
    "\n",
    "# 2. Keep only the top 10 most relevant content tags per book (N=10)\n",
    "N_TAGS = 10\n",
    "df_top_tags_per_book = (\n",
    "    df_content_tags.groupby('goodreads_book_id')\n",
    "    .head(N_TAGS)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# Aggregate the top 10 tag names into a single string ('tag_soup')\n",
    "df_tags_grouped = (\n",
    "    df_top_tags_per_book.groupby('goodreads_book_id')['tag_name']\n",
    "    .apply(lambda x: ' '.join(x.astype(str).str.replace(' ', '-'))) # Join, replacing spaces with hyphens for better NLP\n",
    "    .reset_index(name='tag_soup')\n",
    ")\n",
    "\n",
    "print(\"Aggregated Tags Sample:\")\n",
    "print(df_tags_grouped.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b308dc49",
   "metadata": {},
   "source": [
    "Now merging this modified df_tags_grouped table with our main table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d50620a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Final Books Content Data Sample ---\n",
      "   book_id  goodreads_book_id  \\\n",
      "0        1            2767052   \n",
      "1        2                  3   \n",
      "2        3              41865   \n",
      "3        4               2657   \n",
      "4        5               4671   \n",
      "\n",
      "                                               title  \\\n",
      "0            The Hunger Games (The Hunger Games, #1)   \n",
      "1  Harry Potter and the Sorcerer's Stone (Harry P...   \n",
      "2                            Twilight (Twilight, #1)   \n",
      "3                              To Kill a Mockingbird   \n",
      "4                                   The Great Gatsby   \n",
      "\n",
      "                       authors  \\\n",
      "0              Suzanne Collins   \n",
      "1  J.K. Rowling, Mary GrandPrÃ©   \n",
      "2              Stephenie Meyer   \n",
      "3                   Harper Lee   \n",
      "4          F. Scott Fitzgerald   \n",
      "\n",
      "                                            tag_soup  \n",
      "0  young-adult fiction dystopian dystopia fantasy...  \n",
      "1  fantasy young-adult fiction harry-potter books...  \n",
      "2  young-adult fantasy vampires ya fiction parano...  \n",
      "3  classics classic historical-fiction school clÃ ...  \n",
      "4  classics fiction classic books-i-own literatur...  \n",
      "\n",
      "Total rows in final dataframe: 10000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# 3. Final Merge with Main Books Table (using the goodreads_book_id)\n",
    "df = pd.merge(\n",
    "    df,\n",
    "    df_tags_grouped,\n",
    "    on='goodreads_book_id',\n",
    "    how='left' \n",
    ")\n",
    "\n",
    "df['tag_soup'] = df['tag_soup'].fillna('')\n",
    "\n",
    "print(\"--- Final Books Content Data Sample ---\")\n",
    "print(df[['book_id', 'goodreads_book_id', 'title', 'authors', 'tag_soup']].head())\n",
    "\n",
    "print(f\"\\nTotal rows in final dataframe: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6adcaede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Content Soup Sample (Cleaned and Combined Features):\n",
      "                                                      title                                                                                                                                                         content_soup\n",
      "0                   The Hunger Games (The Hunger Games, #1)                         SuzanneCollins TheHungerGames(TheHungerGames #1) young-adult fiction dystopian dystopia fantasy ya science-fiction books-i-own sci-fi series\n",
      "1  Harry Potter and the Sorcerer's Stone (Harry Potter, #1)  J.K.Rowling MaryGrandPrÃ© HarryPotterandtheSorcerer'sStone(HarryPotter #1) fantasy young-adult fiction harry-potter books-i-own ya series favourites magic childrens\n"
     ]
    }
   ],
   "source": [
    "# Assuming df_books_content is your current DataFrame\n",
    "\n",
    "# Function to clean and combine features\n",
    "def clean_and_combine_features(row):\n",
    "    # Ensure authors and title are clean and concatenated\n",
    "    authors = str(row['authors']).replace(' ', '').replace(',', ' ')\n",
    "    title = str(row['title']).replace(' ', '').replace(',', ' ')\n",
    "    \n",
    "    # The tag_soup already has spaces replaced by hyphens, which is good\n",
    "    tags = str(row['tag_soup'])\n",
    "    \n",
    "    # Combine the key features into one string\n",
    "    return authors + ' ' + title + ' ' + tags\n",
    "\n",
    "# Create the new 'content_soup' column\n",
    "df['content_soup'] = df.apply(clean_and_combine_features, axis=1)\n",
    "\n",
    "print(\"\\nContent Soup Sample (Cleaned and Combined Features):\")\n",
    "print(df[['title', 'content_soup']].head(2).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec8ad15",
   "metadata": {},
   "source": [
    "Preparing the ratings.csv data to be used for neural collaborative filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ecca660b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Unique Users: 53424, Total Unique Books: 10000\n",
      "Number of Positive Interactions: 4122111\n",
      "Generating 6183166 candidate samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_239796/1230673392.py:75: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_negative['target'] = 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimization Complete.\n",
      "Total final training samples: 8244222\n",
      "Positive samples: 4122111, Negative samples: 4122111\n",
      "Final Training Data Sample:\n",
      "   user_index  book_index  target\n",
      "0        9487        4438       0\n",
      "1       18721        1009       0\n",
      "2       18649        7712       0\n",
      "3        5011        9169       1\n",
      "4       51440        7302       0\n"
     ]
    }
   ],
   "source": [
    "df_cf_input = pd.read_csv(\"ratings.csv\")\n",
    "# Assuming df_cf_input is your filtered ratings DataFrame\n",
    "\n",
    "# 1a. Create dense ID mapping\n",
    "user_to_index = {original_id: index for index, original_id in enumerate(df_cf_input['user_id'].unique())}\n",
    "book_to_index = {original_id: index for index, original_id in enumerate(df_cf_input['book_id'].unique())}\n",
    "\n",
    "df_cf_input['user_index'] = df_cf_input['user_id'].map(user_to_index)\n",
    "df_cf_input['book_index'] = df_cf_input['book_id'].map(book_to_index)\n",
    "\n",
    "n_users = len(user_to_index)\n",
    "n_books = len(book_to_index)\n",
    "\n",
    "# 1b. Create binary target (only using high ratings as positive)\n",
    "POSITIVE_RATING_THRESHOLD = 4\n",
    "df_positive = df_cf_input[df_cf_input['rating'] >= POSITIVE_RATING_THRESHOLD].copy()\n",
    "df_positive['target'] = 1\n",
    "\n",
    "print(f\"Total Unique Users: {n_users}, Total Unique Books: {n_books}\")\n",
    "print(f\"Number of Positive Interactions: {len(df_positive)}\")\n",
    "\n",
    "\n",
    "\n",
    "# --- Assumptions ---\n",
    "# df_positive, n_users, n_books are defined\n",
    "# NEGATIVE_SAMPLE_RATIO = 1 (or your desired ratio)\n",
    "\n",
    "# 1. Get the set of all known interactions (CRITICAL for fast lookup)\n",
    "all_interactions = set(zip(df_cf_input['user_index'], df_cf_input['book_index']))\n",
    "positive_samples_count = len(df_positive)\n",
    "\n",
    "# 2. Determine the target number of negative samples\n",
    "target_neg_count = positive_samples_count * NEGATIVE_SAMPLE_RATIO\n",
    "\n",
    "# --- OPTIMIZATION STARTS HERE ---\n",
    "\n",
    "# 3. Generate a large pool of random (user, book) candidates\n",
    "# We generate 1.5x the needed amount to account for overlaps with positive interactions\n",
    "OVERSAMPLE_FACTOR = 1.5\n",
    "total_candidates_to_generate = int(target_neg_count * OVERSAMPLE_FACTOR)\n",
    "\n",
    "print(f\"Generating {total_candidates_to_generate} candidate samples...\")\n",
    "\n",
    "# Vectorized generation of random user and book indices\n",
    "random_user_indices = np.random.randint(0, n_users, total_candidates_to_generate)\n",
    "random_book_indices = np.random.randint(0, n_books, total_candidates_to_generate)\n",
    "\n",
    "# Create a temporary DataFrame of candidate negative samples\n",
    "df_candidates = pd.DataFrame({\n",
    "    'user_index': random_user_indices,\n",
    "    'book_index': random_book_indices,\n",
    "})\n",
    "\n",
    "# 4. Filter out any samples that are already in the 'all_interactions' set\n",
    "\n",
    "# Convert the candidates into tuples for fast set lookup\n",
    "candidate_interactions = set(zip(df_candidates['user_index'], df_candidates['book_index']))\n",
    "\n",
    "# Identify which candidates are actual interactions (collisions)\n",
    "# This is a highly optimized set operation\n",
    "valid_neg_interactions = list(candidate_interactions - all_interactions)\n",
    "\n",
    "# 5. Select the required number of valid negative samples\n",
    "# We must ensure we only take the exact number needed (target_neg_count)\n",
    "df_valid_neg = pd.DataFrame(valid_neg_interactions, columns=['user_index', 'book_index'])\n",
    "\n",
    "# Ensure we have enough and slice to the target count\n",
    "if len(df_valid_neg) < target_neg_count:\n",
    "    print(\"\\nWARNING: Not enough unique negative samples generated. Increase OVERSAMPLE_FACTOR.\")\n",
    "    df_negative = df_valid_neg\n",
    "else:\n",
    "    df_negative = df_valid_neg.head(target_neg_count)\n",
    "\n",
    "# Add the target column\n",
    "df_negative['target'] = 0\n",
    "\n",
    "# --- Final Consolidation ---\n",
    "\n",
    "# 6. Combine positive and negative samples\n",
    "df_training_final = pd.concat([df_positive[['user_index', 'book_index', 'target']], df_negative], ignore_index=True)\n",
    "\n",
    "# 7. Shuffle the data\n",
    "df_training_final = df_training_final.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# 8. Print Results\n",
    "print(f\"\\nOptimization Complete.\")\n",
    "print(f\"Total final training samples: {len(df_training_final)}\")\n",
    "print(f\"Positive samples: {len(df_positive)}, Negative samples: {len(df_negative)}\")\n",
    "print(\"Final Training Data Sample:\")\n",
    "print(df_training_final.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12aad795",
   "metadata": {},
   "source": [
    "Now our nerual collaborative filtering step begins\n",
    "Step 1 is to split our data into training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9aa1c9dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Samples: 6595377\n",
      "Validation Samples: 1648845\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Separate the features (inputs) from the target (output)\n",
    "users = df_training_final['user_index'].values\n",
    "books = df_training_final['book_index'].values\n",
    "targets = df_training_final['target'].values\n",
    "\n",
    "# Split into 80% training and 20% validation sets\n",
    "X_train_u, X_val_u, X_train_i, X_val_i, y_train, y_val = train_test_split(\n",
    "    users, books, targets, test_size=0.2, random_state=42, stratify=targets\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining Samples: {len(X_train_u)}\")\n",
    "print(f\"Validation Samples: {len(X_val_u)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c151aa",
   "metadata": {},
   "source": [
    "Step 2 is defining the base ncf model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33b3759",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, Input, Flatten, Concatenate, Dense, Multiply\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Define the model function again (ensure you have n_users and n_books in scope)\n",
    "def build_ncf_model(num_users, num_items, embedding_dim=10):\n",
    "    # --- Input Layers ---\n",
    "    user_input = Input(shape=(1,), name='user_input')\n",
    "    item_input = Input(shape=(1,), name='item_input')\n",
    "\n",
    "    # --- Embedding Layers (Shared for GMF and MLP paths) ---\n",
    "    # GMF Path Embeddings\n",
    "    user_embedding_gmf = Embedding(input_dim=num_users, output_dim=embedding_dim, name='user_gmf_embedding')(user_input)\n",
    "    item_embedding_gmf = Embedding(input_dim=num_items, output_dim=embedding_dim, name='item_gmf_embedding')(item_input)\n",
    "    # MLP Path Embeddings\n",
    "    user_embedding_mlp = Embedding(input_dim=num_users, output_dim=embedding_dim, name='user_mlp_embedding')(user_input)\n",
    "    item_embedding_mlp = Embedding(input_dim=num_items, output_dim=embedding_dim, name='item_mlp_embedding')(item_input)\n",
    "\n",
    "    # --- GMF Path (Linear Interaction) ---\n",
    "    gmf_user_flat = Flatten()(user_embedding_gmf)\n",
    "    gmf_item_flat = Flatten()(item_embedding_gmf)\n",
    "    gmf_interaction = Multiply()([gmf_user_flat, gmf_item_flat])\n",
    "\n",
    "    # --- MLP Path (Non-linear Interaction) ---\n",
    "    mlp_user_flat = Flatten()(user_embedding_mlp)\n",
    "    mlp_item_flat = Flatten()(item_embedding_mlp)\n",
    "    mlp_interaction = Concatenate()([mlp_user_flat, mlp_item_flat])\n",
    "    \n",
    "    # Simple DNN layers for non-linear learning\n",
    "    mlp_layer = Dense(embedding_dim * 2, activation='relu')(mlp_interaction)\n",
    "    mlp_layer = Dense(embedding_dim, activation='relu')(mlp_layer)\n",
    "\n",
    "    # --- Fusion Layer (Combine GMF and MLP) ---\n",
    "    fusion = Concatenate()([gmf_interaction, mlp_layer])\n",
    "    \n",
    "    # --- Prediction Layer ---\n",
    "    # Output uses sigmoid for binary classification (predicting P(interaction))\n",
    "    output = Dense(1, activation='sigmoid', name='output')(fusion)\n",
    "\n",
    "    model = Model(inputs=[user_input, item_input], outputs=output)\n",
    "    return model\n",
    "\n",
    "# Initialize and compile the model\n",
    "EMBEDDING_DIM = 10 # Hyperparameter: determines the size of the latent factors\n",
    "ncf_model = build_ncf_model(n_users, n_books, embedding_dim=EMBEDDING_DIM)\n",
    "ncf_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), \n",
    "                  loss='binary_crossentropy') # Binary Cross-Entropy for binary classification (0 or 1)\n",
    "\n",
    "print(\"\\nNCF Model Architecture Summary:\")\n",
    "ncf_model.summary()\n",
    "\n",
    "# Image of Neural Collaborative Filtering Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499742e7",
   "metadata": {},
   "source": [
    "Step 3 is training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a71a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS = 10 # Start with 10 epochs, adjust based on validation loss\n",
    "\n",
    "history = ncf_model.fit(\n",
    "    [X_train_u, X_train_i], y_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    verbose=1,\n",
    "    validation_data=([X_val_u, X_val_i], y_val)\n",
    ")\n",
    "\n",
    "print(\"\\nModel Training Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0b568f",
   "metadata": {},
   "source": [
    "now predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96965449",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def recommend_ncf_for_user(ncf_model, user_id, df_cf_input, book_to_index, n_books, top_k=10):\n",
    "    \n",
    "    # 1. Map the original user_id to the internal index\n",
    "    user_index = user_to_index.get(user_id)\n",
    "    \n",
    "    if user_index is None:\n",
    "        print(f\"Error: User ID {user_id} not found in the trained dataset.\")\n",
    "        return []\n",
    "\n",
    "    # 2. Identify books the user HAS interacted with (exclude these)\n",
    "    user_interactions = df_cf_input[df_cf_input['user_id'] == user_id]['book_index'].unique()\n",
    "    \n",
    "    # 3. Identify all book indices the user HAS NOT interacted with (candidates)\n",
    "    all_book_indices = np.arange(n_books)\n",
    "    candidate_book_indices = np.setdiff1d(all_book_indices, user_interactions)\n",
    "    \n",
    "    # 4. Prepare input for the model\n",
    "    # The model expects two arrays: (user_indices, book_indices)\n",
    "    user_indices_array = np.full(len(candidate_book_indices), user_index)\n",
    "    \n",
    "    # 5. Predict scores (inference)\n",
    "    predictions = ncf_model.predict([user_indices_array, candidate_book_indices], verbose=0).flatten()\n",
    "    \n",
    "    # 6. Rank the candidates\n",
    "    # Get the indices that would sort the predictions in descending order\n",
    "    top_k_indices = predictions.argsort()[-top_k:][::-1]\n",
    "    \n",
    "    # Get the corresponding book indices (the *internal* IDs)\n",
    "    top_book_indices = candidate_book_indices[top_k_indices]\n",
    "    top_scores = predictions[top_k_indices]\n",
    "    \n",
    "    # 7. Map back to original book_id (the key to your main books table)\n",
    "    \n",
    "    # Create the inverse map (index_to_book)\n",
    "    index_to_book = {v: k for k, v in book_to_index.items()}\n",
    "    \n",
    "    # Map the internal index back to the external book_id\n",
    "    top_book_ids = [index_to_book[idx] for idx in top_book_indices]\n",
    "\n",
    "    # 8. Return results\n",
    "    results = pd.DataFrame({\n",
    "        'original_book_id': top_book_ids,\n",
    "        'ncf_score': top_scores\n",
    "    })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example Usage: Assuming you want recommendations for a reliable user with original user_id=2\n",
    "# Replace 2 with a real user_id from your dataset\n",
    "# user_id_to_test = 2 \n",
    "# recommendations = recommend_ncf_for_user(ncf_model, user_id_to_test, df_cf_input, book_to_index, n_books)\n",
    "# print(f\"\\nRecommendations for User {user_id_to_test}:\\n\", recommendations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78e7667",
   "metadata": {},
   "source": [
    "using the model to get for some user id, the top k books it would highly rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94cb420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Find a Valid User ID to Test ---\n",
    "# Get the first user_id from the set of users the model was trained on\n",
    "user_id_to_test = df_cf_input['user_id'].iloc[0]\n",
    "\n",
    "print(f\"Generating NCF recommendations for User ID: {user_id_to_test}\\n\")\n",
    "\n",
    "# Call your defined recommendation function\n",
    "recommendations_ncf = recommend_ncf_for_user(\n",
    "    ncf_model=ncf_model,\n",
    "    user_id=user_id_to_test,\n",
    "    df_cf_input=df_cf_input,\n",
    "    book_to_index=book_to_index,\n",
    "    n_books=n_books,\n",
    "    top_k=10\n",
    ")\n",
    "\n",
    "print(\"--- Raw NCF Output (Original Book IDs and Scores) ---\")\n",
    "print(recommendations_ncf)\n",
    "\n",
    "# Assuming your main book content table is named df_books_content\n",
    "# Ensure you have loaded or kept df_books_content from earlier steps!\n",
    "\n",
    "# Perform a merge to join the predicted scores with the book details\n",
    "final_recommendations_ncf = pd.merge(\n",
    "    recommendations_ncf,\n",
    "    df_books_content[['book_id', 'title', 'authors', 'average_rating']], # Select key columns\n",
    "    left_on='original_book_id',\n",
    "    right_on='book_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Clean up and display the final result\n",
    "final_recommendations_ncf = final_recommendations_ncf.sort_values(\n",
    "    'ncf_score', \n",
    "    ascending=False\n",
    ").reset_index(drop=True)\n",
    "\n",
    "# Rename for clarity\n",
    "final_recommendations_ncf = final_recommendations_ncf.rename(\n",
    "    columns={'ncf_score': 'Predicted Score (NCF)'}\n",
    ")\n",
    "\n",
    "print(\"\\n--- Final Personalized Recommendations (NCF) ---\")\n",
    "print(final_recommendations_ncf[[\n",
    "    'Predicted Score (NCF)', \n",
    "    'title', \n",
    "    'authors', \n",
    "    'average_rating'\n",
    "]].head(10).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ed6f13",
   "metadata": {},
   "source": [
    "# Content-Based Filtering (CBF) Model Implementation\n",
    "\n",
    "The Content-Based Filtering component solves the critical **New User/New Item Cold Start Problem** by generating recommendations based purely on descriptive features of the books. This ensures that even users with no prior rating history receive relevant suggestions instantly, and new books (with no ratings) can still be recommended.\n",
    "\n",
    "## ðŸŽ¯ Core Mechanism: Item Similarity\n",
    "\n",
    "Instead of relying on user behavior, the CBF model generates **book-to-book similarity scores** using their shared content.\n",
    "\n",
    "### 1. Feature Consolidation (`tag_soup`)\n",
    "Features like **title, authors, and descriptive tags** were aggregated into a single text block (`content_soup` or `tag_soup`) for each book.\n",
    "\n",
    "### 2. Vectorization (TF-IDF)\n",
    "The text features were converted into a **sparse numerical matrix** using the **Term Frequency-Inverse Document Frequency (TF-IDF)** scheme.  \n",
    "- TF-IDF weights **rare, distinguishing tags** (e.g., 'Austen', 'dystopian') higher than **common terms** (e.g., 'book', 'fiction').\n",
    "\n",
    "### 3. Similarity Matrix Generation\n",
    "The final step involves calculating the **Cosine Similarity** between every book's TF-IDF vector:\n",
    "\n",
    "\\[\n",
    "\\text{Similarity}(B_i, B_j) = \\text{CosineSimilarity}(\\text{Vector}_{B_i}, \\text{Vector}_{B_j})\n",
    "\\]\n",
    "\n",
    "The result is a comprehensive **Item Similarity Matrix**, where we can instantly look up the content similarity score between any two books in the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab54327c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting TF-IDF Vectorization...\n",
      "TF-IDF Matrix Shape: (10000, 1164)\n",
      "Calculating Cosine Similarity Matrix (This may take a moment)...\n",
      "Cosine Similarity Matrix Calculation Complete.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "\n",
    "# Assume df is loaded and preprocessed with the 'tag_soup' column\n",
    "\n",
    "print(\"Starting TF-IDF Vectorization...\")\n",
    "\n",
    "# 1. Initialize the TF-IDF Vectorizer\n",
    "# max_df=0.9: Ignore terms that appear in more than 90% of documents (too common)\n",
    "# min_df=5: Ignore terms that appear in fewer than 5 documents (too rare)\n",
    "tfidf = TfidfVectorizer(stop_words='english', max_df=0.9, min_df=5)\n",
    "\n",
    "# Fit the vectorizer to the 'tag_soup' and transform the text into a sparse matrix\n",
    "tfidf_matrix = tfidf.fit_transform(df['tag_soup'])\n",
    "\n",
    "print(f\"TF-IDF Matrix Shape: {tfidf_matrix.shape}\")\n",
    "print(\"Calculating Cosine Similarity Matrix (This may take a moment)...\")\n",
    "\n",
    "# 2. Compute the Cosine Similarity Matrix\n",
    "# The resulting matrix (N x N) holds the similarity score between every pair of books.\n",
    "# This matrix will be the core of your CBF model.\n",
    "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "print(\"Cosine Similarity Matrix Calculation Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffa6a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping series to quickly lookup the internal index from the book_id\n",
    "book_to_index_series = pd.Series(df_books_content.index, index=df_books_content['book_id']).drop_duplicates()\n",
    "\n",
    "def recommend_cbf_by_book_id(book_id, cosine_sim_matrix, df_content, top_k=10):\n",
    "    \n",
    "    # 1. Get the internal index corresponding to the input book_id\n",
    "    try:\n",
    "        idx = book_to_index_series[book_id]\n",
    "    except KeyError:\n",
    "        return f\"Error: Book ID {book_id} not found in content data.\"\n",
    "\n",
    "    # 2. Get the similarity scores for this book against all other books\n",
    "    # The scores are the row/column corresponding to 'idx' in the matrix\n",
    "    sim_scores = list(enumerate(cosine_sim_matrix[idx]))\n",
    "\n",
    "    # 3. Sort the books based on the similarity scores (descending)\n",
    "    # We ignore the first element (index 0) because it's the book itself (similarity = 1.0)\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    sim_scores = sim_scores[1:top_k + 1] \n",
    "\n",
    "    # 4. Get the book indices and similarity scores\n",
    "    book_indices = [i[0] for i in sim_scores]\n",
    "    scores = [i[1] for i in sim_scores]\n",
    "    \n",
    "    # 5. Get the original book IDs and titles for the top recommendations\n",
    "    recommended_books = df_content.iloc[book_indices].copy()\n",
    "    \n",
    "    # 6. Format and return\n",
    "    results = pd.DataFrame({\n",
    "        'original_book_id': recommended_books['book_id'],\n",
    "        'cbf_score': scores,\n",
    "        'title': recommended_books['title'],\n",
    "        'authors': recommended_books['authors']\n",
    "    })\n",
    "    \n",
    "    return results.reset_index(drop=True)\n",
    "\n",
    "# Example Usage: Find books similar to the book with original book_id=1\n",
    "# You need to ensure book_id=1 is in your df_books_content\n",
    "# recommendation_cbf = recommend_cbf_by_book_id(1, cosine_sim, df_books_content)\n",
    "# print(\"\\n--- Content-Based Recommendations for Book ID 1 ---\")\n",
    "# print(recommendation_cbf.to_string())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
