{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbb2fbb2",
   "metadata": {},
   "source": [
    "Importing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54a9cfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57589ba3",
   "metadata": {},
   "source": [
    "trying to get the shape of data and removing unncessary columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed07623",
   "metadata": {},
   "source": [
    "# 1. Data Loading and Cleaning\n",
    "\n",
    "## Theory: Data Preparation\n",
    "Before any machine learning model can be trained, data must be gathered and sanitized. \n",
    "- **Data Ingestion**: We load the raw CSV files into Pandas DataFrames, which are 2D tabular data structures.\n",
    "- **Noise Reduction**: Not all data is useful. Columns like `image_url`, `isbn`, or `best_book_id` (a duplicate identifier) add memory overhead without providing predictive signal. We drop them early.\n",
    "\n",
    "## Code Block Summary\n",
    "The code below performs the following steps:\n",
    "1.  **Loads** `books.csv` (Metadata), `book_tags.csv` (Tag Links), and `tags.csv` (Tag Names).\n",
    "2.  **Drops** irrelevant columns (`work_id`, `isbn`, `image_url` etc.) from the books dataframe.\n",
    "3.  **Displays** the data types to ensure they are correct (e.g., IDs are integers, Ratings are floats).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1c8b9cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "book_id                        int64\n",
       "goodreads_book_id              int64\n",
       "authors                       object\n",
       "original_publication_year    float64\n",
       "original_title                object\n",
       "title                         object\n",
       "average_rating               float64\n",
       "ratings_count                  int64\n",
       "work_ratings_count             int64\n",
       "work_text_reviews_count        int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"books.csv\")\n",
    "df_book_tags = pd.read_csv(\"book_tags.csv\")\n",
    "df_tags = pd.read_csv(\"tags.csv\")\n",
    "# \n",
    "df = df.drop([\n",
    "    'work_id', 'isbn', 'isbn13', 'language_code', 'image_url', 'small_image_url', 'best_book_id', 'books_count','ratings_1', 'ratings_2', 'ratings_3', 'ratings_4', 'ratings_5'\n",
    "],axis = 1)\n",
    "df.dtypes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccf732c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>goodreads_book_id</th>\n",
       "      <th>tag_id</th>\n",
       "      <th>count</th>\n",
       "      <th>tag_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>30574</td>\n",
       "      <td>167697</td>\n",
       "      <td>to-read</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>11305</td>\n",
       "      <td>37174</td>\n",
       "      <td>fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>11557</td>\n",
       "      <td>34173</td>\n",
       "      <td>favorites</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>8717</td>\n",
       "      <td>12986</td>\n",
       "      <td>currently-reading</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>33114</td>\n",
       "      <td>12716</td>\n",
       "      <td>young-adult</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   goodreads_book_id  tag_id   count           tag_name\n",
       "0                  1   30574  167697            to-read\n",
       "1                  1   11305   37174            fantasy\n",
       "2                  1   11557   34173          favorites\n",
       "3                  1    8717   12986  currently-reading\n",
       "4                  1   33114   12716        young-adult"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tags_named = pd.merge(\n",
    "    df_book_tags,\n",
    "    df_tags,\n",
    "    on='tag_id',\n",
    "    how='left' # Keep all tag records, even if a tag_name is missing \n",
    ")\n",
    "df_tags_named.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a670c32",
   "metadata": {},
   "source": [
    "# 2. Feature Engineering: Creating \"Tag Soup\"\n",
    "\n",
    "## Theory: Content Representation\n",
    "To build a **Content-Based Filtering** system, we need a mathematical way to compare books. We cannot compare \"Harry Potter\" to \"The Hobbit\" directly. \n",
    "\n",
    "We use a technique called **Bag of Words (BoW)** where we represent a document (book) as a collection of its descriptive keywords. We call this consolidated string the **\"Tag Soup\"**.\n",
    "\n",
    "## Code Block Summary\n",
    "The following code creates this \"soup\":\n",
    "1.  **Filters Tags**: Removes generic tags like \"to-read\", \"owned\", or \"books\" which describe a user's *status* rather than the book's *content*.\n",
    "2.  **Top-10 Selection**: For every book, selects the top 10 most frequently applied tags.\n",
    "3.  **String Aggregation**: Joins the Author Name, Book Title, and these Top 10 Tags into a single space-separated string called `tag_soup` (e.g., \"JKRowling HarryPotter Fantasy Magic\").\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7c86f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated Tags Sample:\n",
      "   goodreads_book_id                                           tag_soup\n",
      "0                  1  fantasy young-adult fiction harry-potter books...\n",
      "1                  2  fantasy children children-s all-time-favorites...\n",
      "2                  3  fantasy young-adult fiction harry-potter books...\n",
      "3                  5  fantasy young-adult fiction harry-potter books...\n",
      "4                  6  fantasy young-adult fiction harry-potter ya se...\n"
     ]
    }
   ],
   "source": [
    "# List of generic tags to exclude which are not important\n",
    "GENERIC_TAGS_TO_REMOVE = [\n",
    "    'to-read',\n",
    "    'currently-reading',\n",
    "    'owned',\n",
    "    'favorites',\n",
    "    'default',\n",
    "    'ebook',\n",
    "    'kindle',\n",
    "    'library',\n",
    "    'owned-books',\n",
    "    'my-books',\n",
    "    'books'\n",
    "]\n",
    "\n",
    "# 1. Filter out generic tags\n",
    "df_content_tags = df_tags_named[~df_tags_named['tag_name'].isin(GENERIC_TAGS_TO_REMOVE)].copy()\n",
    "\n",
    "# Sort by count  to prioritize the most important tags for each book\n",
    "df_content_tags = df_content_tags.sort_values(\n",
    "    ['goodreads_book_id', 'count'],\n",
    "    ascending=[True, False]\n",
    ")\n",
    "\n",
    "# 2. Keep only the top 10 most relevant content tags per book (N=10)\n",
    "N_TAGS = 10\n",
    "df_top_tags_per_book = (\n",
    "    df_content_tags.groupby('goodreads_book_id')\n",
    "    .head(N_TAGS)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# Aggregate the top 10 tag names into a single string ('tag_soup')\n",
    "df_tags_grouped = (\n",
    "    df_top_tags_per_book.groupby('goodreads_book_id')['tag_name']\n",
    "    .apply(lambda x: ' '.join(x.astype(str).str.replace(' ', '-'))) # Join, replacing spaces with hyphens for better NLP\n",
    "    .reset_index(name='tag_soup')\n",
    ")\n",
    "\n",
    "print(\"Aggregated Tags Sample:\")\n",
    "print(df_tags_grouped.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b308dc49",
   "metadata": {},
   "source": [
    "Now merging this modified df_tags_grouped table with our main table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d50620a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Final Books Content Data Sample ---\n",
      "   book_id  goodreads_book_id  \\\n",
      "0        1            2767052   \n",
      "1        2                  3   \n",
      "2        3              41865   \n",
      "3        4               2657   \n",
      "4        5               4671   \n",
      "\n",
      "                                               title  \\\n",
      "0            The Hunger Games (The Hunger Games, #1)   \n",
      "1  Harry Potter and the Sorcerer's Stone (Harry P...   \n",
      "2                            Twilight (Twilight, #1)   \n",
      "3                              To Kill a Mockingbird   \n",
      "4                                   The Great Gatsby   \n",
      "\n",
      "                       authors  \\\n",
      "0              Suzanne Collins   \n",
      "1  J.K. Rowling, Mary GrandPré   \n",
      "2              Stephenie Meyer   \n",
      "3                   Harper Lee   \n",
      "4          F. Scott Fitzgerald   \n",
      "\n",
      "                                            tag_soup  \n",
      "0  young-adult fiction dystopian dystopia fantasy...  \n",
      "1  fantasy young-adult fiction harry-potter books...  \n",
      "2  young-adult fantasy vampires ya fiction parano...  \n",
      "3  classics classic historical-fiction school clà...  \n",
      "4  classics fiction classic books-i-own literatur...  \n",
      "\n",
      "Total rows in final dataframe: 10000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# 3. Final Merge with Main Books Table (using the goodreads_book_id)\n",
    "df = pd.merge(\n",
    "    df,\n",
    "    df_tags_grouped,\n",
    "    on='goodreads_book_id',\n",
    "    how='left' \n",
    ")\n",
    "\n",
    "df['tag_soup'] = df['tag_soup'].fillna('')\n",
    "\n",
    "print(\"--- Final Books Content Data Sample ---\")\n",
    "print(df[['book_id', 'goodreads_book_id', 'title', 'authors', 'tag_soup']].head())\n",
    "\n",
    "print(f\"\\nTotal rows in final dataframe: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6adcaede",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book_id</th>\n",
       "      <th>goodreads_book_id</th>\n",
       "      <th>authors</th>\n",
       "      <th>original_publication_year</th>\n",
       "      <th>original_title</th>\n",
       "      <th>title</th>\n",
       "      <th>average_rating</th>\n",
       "      <th>ratings_count</th>\n",
       "      <th>work_ratings_count</th>\n",
       "      <th>work_text_reviews_count</th>\n",
       "      <th>tag_soup</th>\n",
       "      <th>content_soup</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2767052</td>\n",
       "      <td>Suzanne Collins</td>\n",
       "      <td>2008.0</td>\n",
       "      <td>The Hunger Games</td>\n",
       "      <td>The Hunger Games (The Hunger Games, #1)</td>\n",
       "      <td>4.34</td>\n",
       "      <td>4780653</td>\n",
       "      <td>4942365</td>\n",
       "      <td>155254</td>\n",
       "      <td>young-adult fiction dystopian dystopia fantasy...</td>\n",
       "      <td>SuzanneCollins TheHungerGames(TheHungerGames #...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>J.K. Rowling, Mary GrandPré</td>\n",
       "      <td>1997.0</td>\n",
       "      <td>Harry Potter and the Philosopher's Stone</td>\n",
       "      <td>Harry Potter and the Sorcerer's Stone (Harry P...</td>\n",
       "      <td>4.44</td>\n",
       "      <td>4602479</td>\n",
       "      <td>4800065</td>\n",
       "      <td>75867</td>\n",
       "      <td>fantasy young-adult fiction harry-potter books...</td>\n",
       "      <td>J.K.Rowling MaryGrandPré HarryPotterandtheSorc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>41865</td>\n",
       "      <td>Stephenie Meyer</td>\n",
       "      <td>2005.0</td>\n",
       "      <td>Twilight</td>\n",
       "      <td>Twilight (Twilight, #1)</td>\n",
       "      <td>3.57</td>\n",
       "      <td>3866839</td>\n",
       "      <td>3916824</td>\n",
       "      <td>95009</td>\n",
       "      <td>young-adult fantasy vampires ya fiction parano...</td>\n",
       "      <td>StephenieMeyer Twilight(Twilight #1) young-adu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2657</td>\n",
       "      <td>Harper Lee</td>\n",
       "      <td>1960.0</td>\n",
       "      <td>To Kill a Mockingbird</td>\n",
       "      <td>To Kill a Mockingbird</td>\n",
       "      <td>4.25</td>\n",
       "      <td>3198671</td>\n",
       "      <td>3340896</td>\n",
       "      <td>72586</td>\n",
       "      <td>classics classic historical-fiction school clà...</td>\n",
       "      <td>HarperLee ToKillaMockingbird classics classic ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>4671</td>\n",
       "      <td>F. Scott Fitzgerald</td>\n",
       "      <td>1925.0</td>\n",
       "      <td>The Great Gatsby</td>\n",
       "      <td>The Great Gatsby</td>\n",
       "      <td>3.89</td>\n",
       "      <td>2683664</td>\n",
       "      <td>2773745</td>\n",
       "      <td>51992</td>\n",
       "      <td>classics fiction classic books-i-own literatur...</td>\n",
       "      <td>F.ScottFitzgerald TheGreatGatsby classics fict...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   book_id  goodreads_book_id                      authors  \\\n",
       "0        1            2767052              Suzanne Collins   \n",
       "1        2                  3  J.K. Rowling, Mary GrandPré   \n",
       "2        3              41865              Stephenie Meyer   \n",
       "3        4               2657                   Harper Lee   \n",
       "4        5               4671          F. Scott Fitzgerald   \n",
       "\n",
       "   original_publication_year                            original_title  \\\n",
       "0                     2008.0                          The Hunger Games   \n",
       "1                     1997.0  Harry Potter and the Philosopher's Stone   \n",
       "2                     2005.0                                  Twilight   \n",
       "3                     1960.0                     To Kill a Mockingbird   \n",
       "4                     1925.0                          The Great Gatsby   \n",
       "\n",
       "                                               title  average_rating  \\\n",
       "0            The Hunger Games (The Hunger Games, #1)            4.34   \n",
       "1  Harry Potter and the Sorcerer's Stone (Harry P...            4.44   \n",
       "2                            Twilight (Twilight, #1)            3.57   \n",
       "3                              To Kill a Mockingbird            4.25   \n",
       "4                                   The Great Gatsby            3.89   \n",
       "\n",
       "   ratings_count  work_ratings_count  work_text_reviews_count  \\\n",
       "0        4780653             4942365                   155254   \n",
       "1        4602479             4800065                    75867   \n",
       "2        3866839             3916824                    95009   \n",
       "3        3198671             3340896                    72586   \n",
       "4        2683664             2773745                    51992   \n",
       "\n",
       "                                            tag_soup  \\\n",
       "0  young-adult fiction dystopian dystopia fantasy...   \n",
       "1  fantasy young-adult fiction harry-potter books...   \n",
       "2  young-adult fantasy vampires ya fiction parano...   \n",
       "3  classics classic historical-fiction school clà...   \n",
       "4  classics fiction classic books-i-own literatur...   \n",
       "\n",
       "                                        content_soup  \n",
       "0  SuzanneCollins TheHungerGames(TheHungerGames #...  \n",
       "1  J.K.Rowling MaryGrandPré HarryPotterandtheSorc...  \n",
       "2  StephenieMeyer Twilight(Twilight #1) young-adu...  \n",
       "3  HarperLee ToKillaMockingbird classics classic ...  \n",
       "4  F.ScottFitzgerald TheGreatGatsby classics fict...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assuming df_books_content is your current DataFrame\n",
    "\n",
    "# Function to clean and combine features\n",
    "def clean_and_combine_features(row):\n",
    "    # Ensure authors and title are clean and concatenated\n",
    "    authors = str(row['authors']).replace(' ', '').replace(',', ' ')\n",
    "    title = str(row['title']).replace(' ', '').replace(',', ' ')\n",
    "    \n",
    "    \n",
    "    tags = str(row['tag_soup'])\n",
    "    \n",
    "    # Combine the key features into one string\n",
    "    return authors + ' ' + title + ' ' + tags\n",
    "\n",
    "# Create the new 'content_soup' column\n",
    "df['content_soup'] = df.apply(clean_and_combine_features, axis=1)\n",
    "\n",
    "# print(\"\\nContent Soup Sample (Cleaned and Combined Features):\")\n",
    "# print(df[['title', 'content_soup']].head(2).to_string())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec8ad15",
   "metadata": {},
   "source": [
    "Preparing the ratings.csv data to be used for neural collaborative filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0277f400",
   "metadata": {},
   "source": [
    "# 3. Collaborative Filtering Preparation\n",
    "\n",
    "## Theory: Neural Collaborative Filtering (NCF)\n",
    "Collaborative filtering relies on the assumption that *users who agreed in the past will agree in the future*.\n",
    "- **Implicit Feedback**: In many real-world scenarios, we don't have explicit \"dislikes\". We only know what a user *did* read (Positive). We assume that if a user *didn't* read a popular book, they might not be interested (Negative), or simply haven't seen it yet.\n",
    "- **Negative Sampling**: To train a classifier to distinguish \"Like\" (1) from \"Dislike\" (0), we must artificially generate \"Negative\" examples by pairing users with random books they haven't interacted with.\n",
    "\n",
    "## Code Block Summary\n",
    "The code prepares the training data:\n",
    "1.  **ID Mapping**: Converts `user_id` and `book_id` into contiguous integers (0 to N) for embedding layer lookup.\n",
    "2.  **Positives**: Selects all user-book pairs with rating >= 4.\n",
    "3.  **Negatives**: Generates random (user, book) pairs that do *not* exist in the training set, assigning them a target of 0.\n",
    "4.  **Balance**: Ensures a 1:1 ratio of Positives to Negatives for balanced training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ecca660b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Unique Users: 53424, Total Unique Books: 10000\n",
      "Number of Positive Interactions: 4122111\n",
      "Generating 6183166 candidate samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_85824/1620348602.py:73: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_negative['target'] = 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimization Complete.\n",
      "Total final training samples: 8244222\n",
      "Positive samples: 4122111, Negative samples: 4122111\n",
      "Final Training Data Sample:\n",
      "   user_index  book_index  target\n",
      "0       41461        8109       1\n",
      "1       19103        2614       1\n",
      "2       24625        9347       1\n",
      "3       22435        4598       0\n",
      "4       14989        1091       0\n"
     ]
    }
   ],
   "source": [
    "df_cf_input = pd.read_csv(\"ratings.csv\")\n",
    "\n",
    "# Create dense ID mapping\n",
    "user_to_index = {original_id: index for index, original_id in enumerate(df_cf_input['user_id'].unique())}\n",
    "book_to_index = {original_id: index for index, original_id in enumerate(df_cf_input['book_id'].unique())}\n",
    "\n",
    "df_cf_input['user_index'] = df_cf_input['user_id'].map(user_to_index)\n",
    "df_cf_input['book_index'] = df_cf_input['book_id'].map(book_to_index)\n",
    "\n",
    "n_users = len(user_to_index)\n",
    "n_books = len(book_to_index)\n",
    "\n",
    "#  Create binary target (only using high ratings as positive)\n",
    "POSITIVE_RATING_THRESHOLD = 4\n",
    "df_positive = df_cf_input[df_cf_input['rating'] >= POSITIVE_RATING_THRESHOLD].copy()\n",
    "df_positive['target'] = 1\n",
    "\n",
    "print(f\"Total Unique Users: {n_users}, Total Unique Books: {n_books}\")\n",
    "print(f\"Number of Positive Interactions: {len(df_positive)}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 1. Get the set of all known interactions\n",
    "all_interactions = set(zip(df_cf_input['user_index'], df_cf_input['book_index']))\n",
    "positive_samples_count = len(df_positive)\n",
    "\n",
    "# 2. Determine the target number of negative samples\n",
    "NEGATIVE_SAMPLE_RATIO=1\n",
    "target_neg_count = positive_samples_count * NEGATIVE_SAMPLE_RATIO\n",
    "\n",
    "\n",
    "\n",
    "# 3. Generate a large pool of random (user, book) candidates\n",
    "# We generate 1.5x the needed amount to account for overlaps with positive interactions\n",
    "OVERSAMPLE_FACTOR = 1.5\n",
    "total_candidates_to_generate = int(target_neg_count * OVERSAMPLE_FACTOR)\n",
    "\n",
    "print(f\"Generating {total_candidates_to_generate} candidate samples...\")\n",
    "\n",
    "# Vectorized generation of random user and book indices\n",
    "random_user_indices = np.random.randint(0, n_users, total_candidates_to_generate)\n",
    "random_book_indices = np.random.randint(0, n_books, total_candidates_to_generate)\n",
    "\n",
    "# Create a temporary DataFrame of candidate negative samples\n",
    "df_candidates = pd.DataFrame({\n",
    "    'user_index': random_user_indices,\n",
    "    'book_index': random_book_indices,\n",
    "})\n",
    "\n",
    "# 4. Filter out any samples that are already in the 'all_interactions' set\n",
    "\n",
    "# Convert the candidates into tuples for fast set lookup\n",
    "candidate_interactions = set(zip(df_candidates['user_index'], df_candidates['book_index']))\n",
    "\n",
    "# Identify which candidates are actual interactions (collisions)\n",
    "# This is a highly optimized set operation\n",
    "valid_neg_interactions = list(candidate_interactions - all_interactions)\n",
    "\n",
    "# 5. Select the required number of valid negative samples\n",
    "# We must ensure we only take the exact number needed (target_neg_count)\n",
    "df_valid_neg = pd.DataFrame(valid_neg_interactions, columns=['user_index', 'book_index'])\n",
    "\n",
    "# Ensure we have enough and slice to the target count\n",
    "if len(df_valid_neg) < target_neg_count:\n",
    "    print(\"\\nWARNING: Not enough unique negative samples generated. Increase OVERSAMPLE_FACTOR.\")\n",
    "    df_negative = df_valid_neg\n",
    "else:\n",
    "    df_negative = df_valid_neg.head(target_neg_count)\n",
    "\n",
    "# Add the target column\n",
    "df_negative['target'] = 0\n",
    "\n",
    "# --- Final Consolidation ---\n",
    "\n",
    "# 6. Combine positive and negative samples\n",
    "df_training_final = pd.concat([df_positive[['user_index', 'book_index', 'target']], df_negative], ignore_index=True)\n",
    "\n",
    "# 7. Shuffle the data\n",
    "df_training_final = df_training_final.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# 8. Print Results\n",
    "print(f\"\\nOptimization Complete.\")\n",
    "print(f\"Total final training samples: {len(df_training_final)}\")\n",
    "print(f\"Positive samples: {len(df_positive)}, Negative samples: {len(df_negative)}\")\n",
    "print(\"Final Training Data Sample:\")\n",
    "print(df_training_final.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12aad795",
   "metadata": {},
   "source": [
    "Now our nerual collaborative filtering step begins\n",
    "Step 1 is to split our data into training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9aa1c9dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Samples: 6595377\n",
      "Validation Samples: 1648845\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Separate the features (inputs) from the target (output)\n",
    "users = df_training_final['user_index'].values\n",
    "books = df_training_final['book_index'].values\n",
    "targets = df_training_final['target'].values\n",
    "\n",
    "# Split into 80% training and 20% validation sets\n",
    "X_train_u, X_val_u, X_train_i, X_val_i, y_train, y_val = train_test_split(\n",
    "    users, books, targets, test_size=0.2, random_state=42, stratify=targets\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining Samples: {len(X_train_u)}\")\n",
    "print(f\"Validation Samples: {len(X_val_u)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c151aa",
   "metadata": {},
   "source": [
    "Step 2 is defining the base ncf model architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc18e2ec",
   "metadata": {},
   "source": [
    "# 4. Neural Collaborative Filtering (NCF) Model Architecture\n",
    "\n",
    "## Theory: The NeuMF Architecture\n",
    "We use the **NeuMF (Neural Matrix Factorization)** framework which fuses two powerful concepts:\n",
    "1.  **GMF (Generalized Matrix Factorization)**: A neural version of the classic Dot Product. It effectively captures linear relationships between users and items.\n",
    "2.  **MLP (Multi-Layer Perceptron)**: A standard deep feed-forward network. It uses non-linear activation functions (ReLU) to learn complex, non-obvious patterns in the data.\n",
    "\n",
    "## Code Block Summary\n",
    "The `build_ncf_model` function defines the Keras graph:\n",
    "-   **Inputs**: Two integers (`user_input`, `item_input`).\n",
    "-   **Embeddings**: Learns a dense vector representation (size 10) for every user and book. \n",
    "-   **Core Layers**: \n",
    "    -   GMF path calculates `ElementWiseProduct(UserEmb, ItemEmb)`.\n",
    "    -   MLP path calculates `Concatenate(UserEmb, ItemEmb)` -> Dense Layers.\n",
    "-   **Output**: A final Sigmoid layer predicts a probability between 0.0 and 1.0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d33b3759",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-17 19:00:31.924168: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NCF Model Architecture Summary:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-17 19:00:36.936475: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ user_input          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ item_input          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ user_mlp_embedding  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">534,240</span> │ user_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ item_mlp_embedding  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">100,000</span> │ item_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ user_mlp_embeddi… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ item_mlp_embeddi… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ user_gmf_embedding  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">534,240</span> │ user_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ item_gmf_embedding  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">100,000</span> │ item_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ flatten_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ flatten_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ user_gmf_embeddi… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ item_gmf_embeddi… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">420</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multiply (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Multiply</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ flatten[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    │\n",
       "│                     │                   │            │ flatten_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">210</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multiply[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span> │ concatenate_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ user_input          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ item_input          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ user_mlp_embedding  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m10\u001b[0m)     │    \u001b[38;5;34m534,240\u001b[0m │ user_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ item_mlp_embedding  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m10\u001b[0m)     │    \u001b[38;5;34m100,000\u001b[0m │ item_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten_2 (\u001b[38;5;33mFlatten\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ user_mlp_embeddi… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten_3 (\u001b[38;5;33mFlatten\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ item_mlp_embeddi… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ user_gmf_embedding  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m10\u001b[0m)     │    \u001b[38;5;34m534,240\u001b[0m │ user_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ item_gmf_embedding  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m10\u001b[0m)     │    \u001b[38;5;34m100,000\u001b[0m │ item_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ flatten_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ flatten_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ user_gmf_embeddi… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ item_gmf_embeddi… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)        │        \u001b[38;5;34m420\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multiply (\u001b[38;5;33mMultiply\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ flatten[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],    │\n",
       "│                     │                   │            │ flatten_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)        │        \u001b[38;5;34m210\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ multiply[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ output (\u001b[38;5;33mDense\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │         \u001b[38;5;34m21\u001b[0m │ concatenate_1[\u001b[38;5;34m0\u001b[0m]… │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,269,131</span> (4.84 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,269,131\u001b[0m (4.84 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,269,131</span> (4.84 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,269,131\u001b[0m (4.84 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, Input, Flatten, Concatenate, Dense, Multiply\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Define the model function again \n",
    "def build_ncf_model(num_users, num_items, embedding_dim=10):\n",
    "    # --- Input Layers ---\n",
    "    user_input = Input(shape=(1,), name='user_input')\n",
    "    item_input = Input(shape=(1,), name='item_input')\n",
    "\n",
    "    # --- Embedding Layers (Shared for GMF and MLP paths) ---\n",
    "    # GMF Path Embeddings\n",
    "    user_embedding_gmf = Embedding(input_dim=num_users, output_dim=embedding_dim, name='user_gmf_embedding')(user_input)\n",
    "    item_embedding_gmf = Embedding(input_dim=num_items, output_dim=embedding_dim, name='item_gmf_embedding')(item_input)\n",
    "    # MLP Path Embeddings\n",
    "    user_embedding_mlp = Embedding(input_dim=num_users, output_dim=embedding_dim, name='user_mlp_embedding')(user_input)\n",
    "    item_embedding_mlp = Embedding(input_dim=num_items, output_dim=embedding_dim, name='item_mlp_embedding')(item_input)\n",
    "\n",
    "    # --- GMF Path (Linear Interaction) ---\n",
    "    gmf_user_flat = Flatten()(user_embedding_gmf)\n",
    "    gmf_item_flat = Flatten()(item_embedding_gmf)\n",
    "    gmf_interaction = Multiply()([gmf_user_flat, gmf_item_flat])\n",
    "\n",
    "    # --- MLP Path (Non-linear Interaction) ---\n",
    "    mlp_user_flat = Flatten()(user_embedding_mlp)\n",
    "    mlp_item_flat = Flatten()(item_embedding_mlp)\n",
    "    mlp_interaction = Concatenate()([mlp_user_flat, mlp_item_flat])\n",
    "    \n",
    "    # Simple DNN layers for non-linear learning\n",
    "    mlp_layer = Dense(embedding_dim * 2, activation='relu')(mlp_interaction)\n",
    "    mlp_layer = Dense(embedding_dim, activation='relu')(mlp_layer)\n",
    "\n",
    "    # --- Fusion Layer (Combine GMF and MLP) ---\n",
    "    fusion = Concatenate()([gmf_interaction, mlp_layer])\n",
    "    \n",
    "    # --- Prediction Layer ---\n",
    "    # Output uses sigmoid for binary classification (predicting P(interaction))\n",
    "    output = Dense(1, activation='sigmoid', name='output')(fusion)\n",
    "\n",
    "    model = Model(inputs=[user_input, item_input], outputs=output)\n",
    "    return model\n",
    "\n",
    "# Initialize and compile the model\n",
    "EMBEDDING_DIM = 10 # Hyperparameter: determines the size of the latent factors\n",
    "ncf_model = build_ncf_model(n_users, n_books, embedding_dim=EMBEDDING_DIM)\n",
    "ncf_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), \n",
    "                  loss='binary_crossentropy') # Binary Cross-Entropy for binary classification (0 or 1)\n",
    "\n",
    "print(\"\\nNCF Model Architecture Summary:\")\n",
    "ncf_model.summary()\n",
    "\n",
    "# Image of Neural Collaborative Filtering Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499742e7",
   "metadata": {},
   "source": [
    "Step 3 is training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fd612d",
   "metadata": {},
   "source": [
    "# 5. Model Training\n",
    "\n",
    "## Theory: Binary Classification Training\n",
    "Since our output is a probability (0-1), we use **Binary Cross-Entropy Loss** (Log Loss). This penalizes the model heavily if it confidently predicts the wrong class (e.g., predicting 0.9 for a Negative sample).\n",
    "\n",
    "## Code Block Summary\n",
    "1.  **Compile**: Sets the optimizer (Adam, learning_rate=0.001) and loss function.\n",
    "2.  **Fit**: Trains the model on the (User, Book) pairs against the Target (0 or 1).\n",
    "3.  **Epochs**: Iterates through the data 3 times to refine the weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7a71a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing model from ncf_model.keras...\n",
      "Model loaded successfully.\n",
      "Model ready.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "MODEL_PATH = \"ncf_model.keras\"\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS = 5\n",
    "\n",
    "if os.path.exists(MODEL_PATH):\n",
    "    print(f\"Loading existing model from {MODEL_PATH}...\")\n",
    "    try:\n",
    "        ncf_model = load_model(MODEL_PATH)\n",
    "        print(\"Model loaded successfully.\")\n",
    "        history = None\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        print(\"Retraining model...\")\n",
    "        os.remove(MODEL_PATH)\n",
    "\n",
    "        history = ncf_model.fit(\n",
    "            [X_train_u, X_train_i], y_train,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            epochs=EPOCHS,\n",
    "            verbose=1,\n",
    "            validation_data=([X_val_u, X_val_i], y_val)\n",
    "        )\n",
    "\n",
    "        ncf_model.save(MODEL_PATH)\n",
    "\n",
    "else:\n",
    "    print(\"No saved model found. Training from scratch...\")\n",
    "\n",
    "    history = ncf_model.fit(\n",
    "        [X_train_u, X_train_i], y_train,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=EPOCHS,\n",
    "        verbose=1,\n",
    "        validation_data=([X_val_u, X_val_i], y_val)\n",
    "    )\n",
    "\n",
    "    ncf_model.save(MODEL_PATH)\n",
    "\n",
    "print(\"Model ready.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0b568f",
   "metadata": {},
   "source": [
    "now predicting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3d16fa",
   "metadata": {},
   "source": [
    "# 6. Generating Recommendations (Inference)\n",
    "\n",
    "## Theory: Ranking Unseen Items\n",
    "The ultimate goal of a recommender is to find the \"needle in the haystack\" of unread books. \n",
    "For a specific user, we conceptually \"ask\" the model: *\"If this user were to see Book X, what is the probability they would like it?\"*\n",
    "\n",
    "## Code Block Summary\n",
    "The `recommend_ncf_for_user` function:\n",
    "1.  **Filters**: Identifies all books the user has *not* read yet (Candidates).\n",
    "2.  **Predicts**: Passes the (User_ID, Candidate_Book_IDs) batch into the trained NCF model.\n",
    "3.  **Sorts**: Ranks the candidates by their predicted probability score (Descending).\n",
    "4.  **Returns**: The list of top K books with the highest predicted scores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96965449",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "def recommend_ncf_for_user(ncf_model, user_id, df_cf_input, book_to_index, n_books, top_k=10):\n",
    "    \n",
    "    # 1. Map the original user_id to the internal index\n",
    "    user_index = user_to_index.get(user_id)\n",
    "    \n",
    "    if user_index is None:\n",
    "        print(f\"Error: User ID {user_id} not found in the trained dataset.\")\n",
    "        return []\n",
    "\n",
    "    # 2. Identify books the user HAS interacted with (exclude these)\n",
    "    user_interactions = df_cf_input[df_cf_input['user_id'] == user_id]['book_index'].unique()\n",
    "    \n",
    "    # 3. Identify all book indices the user HAS NOT interacted with (candidates)\n",
    "    all_book_indices = np.arange(n_books)\n",
    "    candidate_book_indices = np.setdiff1d(all_book_indices, user_interactions)\n",
    "    \n",
    "    # 4. Prepare input for the model\n",
    "    # The model expects two arrays: (user_indices, book_indices)\n",
    "    user_indices_array = np.full(len(candidate_book_indices), user_index)\n",
    "    \n",
    "    # 5. Predict scores (inference)\n",
    "    predictions = ncf_model.predict([user_indices_array, candidate_book_indices], verbose=0).flatten()\n",
    "    \n",
    "    # 6. Rank the candidates\n",
    "    # Get the indices that would sort the predictions in descending order\n",
    "    top_k_indices = predictions.argsort()[-top_k:][::-1]\n",
    "    \n",
    "    # Get the corresponding book indices (the *internal* IDs)\n",
    "    top_book_indices = candidate_book_indices[top_k_indices]\n",
    "    top_scores = predictions[top_k_indices]\n",
    "    \n",
    "    # 7. Map back to original book_id (the key to your main books table)\n",
    "    \n",
    "    # Create the inverse map (index_to_book)\n",
    "    index_to_book = {v: k for k, v in book_to_index.items()}\n",
    "    \n",
    "    # Map the internal index back to the external book_id\n",
    "    top_book_ids = [index_to_book[idx] for idx in top_book_indices]\n",
    "\n",
    "    # 8. Return results\n",
    "    results = pd.DataFrame({\n",
    "        'original_book_id': top_book_ids,\n",
    "        'ncf_score': top_scores\n",
    "    })\n",
    "    \n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78e7667",
   "metadata": {},
   "source": [
    "using the model to get for some user id, the top k books it would highly rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f94cb420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating NCF recommendations for User ID: 1\n",
      "\n",
      "--- Raw NCF Output (Original Book IDs and Scores) ---\n",
      "   original_book_id  ncf_score\n",
      "0                 8   0.995273\n",
      "1                 5   0.994915\n",
      "2               291   0.994778\n",
      "3               658   0.994190\n",
      "4               172   0.994183\n",
      "5               270   0.993441\n",
      "6               118   0.992670\n",
      "7                14   0.991895\n",
      "8               895   0.990396\n",
      "9               820   0.989482\n",
      "\n",
      "--- Final Personalized Recommendations (NCF) ---\n",
      "   Predicted Score (NCF)                         title                                               authors  average_rating\n",
      "0               0.995273        The Catcher in the Rye                                         J.D. Salinger            3.79\n",
      "1               0.994915              The Great Gatsby                                   F. Scott Fitzgerald            3.89\n",
      "2               0.994778             Cutting for Stone                                      Abraham Verghese            4.28\n",
      "3               0.994190               The Corrections                                      Jonathan Franzen            3.78\n",
      "4               0.994183                 Anna Karenina  Leo Tolstoy, Louise Maude, Leo Tolstoj, Aylmer Maude            4.02\n",
      "5               0.993441                       Rebecca                      Daphne du Maurier, Sally Beauman            4.20\n",
      "6               0.992670             The Joy Luck Club                                               Amy Tan            3.90\n",
      "7               0.991895                   Animal Farm                                         George Orwell            3.87\n",
      "8               0.990396  The Year of Magical Thinking                                           Joan Didion            3.86\n",
      "9               0.989482        The Remains of the Day                                        Kazuo Ishiguro            4.10\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get the first user_id from the set of users the model was trained on\n",
    "user_id_to_test = df_cf_input['user_id'].iloc[0]\n",
    "\n",
    "print(f\"Generating NCF recommendations for User ID: {user_id_to_test}\\n\")\n",
    "\n",
    "# Call your defined recommendation function\n",
    "recommendations_ncf = recommend_ncf_for_user(\n",
    "    ncf_model=ncf_model,\n",
    "    user_id=user_id_to_test,\n",
    "    df_cf_input=df_cf_input,\n",
    "    book_to_index=book_to_index,\n",
    "    n_books=n_books,\n",
    "    top_k=10\n",
    ")\n",
    "\n",
    "print(\"--- Raw NCF Output (Original Book IDs and Scores) ---\")\n",
    "print(recommendations_ncf)\n",
    "\n",
    "\n",
    "\n",
    "# Perform a merge to join the predicted scores with the book details\n",
    "final_recommendations_ncf = pd.merge(\n",
    "    recommendations_ncf,\n",
    "    df[['book_id', 'title', 'authors', 'average_rating']], # Select key columns\n",
    "    left_on='original_book_id',\n",
    "    right_on='book_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Clean up and display the final result\n",
    "final_recommendations_ncf = final_recommendations_ncf.sort_values(\n",
    "    'ncf_score', \n",
    "    ascending=False\n",
    ").reset_index(drop=True)\n",
    "\n",
    "# Rename for clarity\n",
    "final_recommendations_ncf = final_recommendations_ncf.rename(\n",
    "    columns={'ncf_score': 'Predicted Score (NCF)'}\n",
    ")\n",
    "\n",
    "print(\"\\n--- Final Personalized Recommendations (NCF) ---\")\n",
    "print(final_recommendations_ncf[[\n",
    "    'Predicted Score (NCF)', \n",
    "    'title', \n",
    "    'authors', \n",
    "    'average_rating'\n",
    "]].head(10).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ed6f13",
   "metadata": {},
   "source": [
    "# Content-Based Filtering (CBF) Model Implementation\n",
    "\n",
    "The Content-Based Filtering component solves the critical **New User/New Item Cold Start Problem** by generating recommendations based purely on descriptive features of the books. This ensures that even users with no prior rating history receive relevant suggestions instantly, and new books (with no ratings) can still be recommended.\n",
    "\n",
    "## 🎯 Core Mechanism: Item Similarity\n",
    "\n",
    "Instead of relying on user behavior, the CBF model generates **book-to-book similarity scores** using their shared content.\n",
    "\n",
    "### 1. Feature Consolidation (`tag_soup`)\n",
    "Features like **title, authors, and descriptive tags** were aggregated into a single text block (`content_soup` or `tag_soup`) for each book.\n",
    "\n",
    "### 2. Vectorization (TF-IDF)\n",
    "The text features were converted into a **sparse numerical matrix** using the **Term Frequency-Inverse Document Frequency (TF-IDF)** scheme.  \n",
    "- TF-IDF weights **rare, distinguishing tags** (e.g., 'Austen', 'dystopian') higher than **common terms** (e.g., 'book', 'fiction').\n",
    "\n",
    "### 3. Similarity Matrix Generation\n",
    "The final step involves calculating the **Cosine Similarity** between every book's TF-IDF vector:\n",
    "\n",
    "\\[\n",
    "\\text{Similarity}(B_i, B_j) = \\text{CosineSimilarity}(\\text{Vector}_{B_i}, \\text{Vector}_{B_j})\n",
    "\\]\n",
    "\n",
    "The result is a comprehensive **Item Similarity Matrix**, where we can instantly look up the content similarity score between any two books in the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959a15f8",
   "metadata": {},
   "source": [
    "# 7. Content-Based Filtering (CBF) Theory\n",
    "\n",
    "## Theory: TF-IDF and Cosine Similarity\n",
    "**TF-IDF (Term Frequency - Inverse Document Frequency)** is a statistical measure used to evaluate the importance of a word to a document in a corpus.\n",
    "-   **TF**: High if a word appears often in a specific book (e.g., \"Vampire\" in Twilight).\n",
    "-   **IDF**: High if a word is rare across *all* books. This lowers the weight of common words like \"Story\" or \"Book\".\n",
    "\n",
    "**Cosine Similarity** measures the angle between two vectors. \n",
    "-   If vectors point in the same direction (Angle=0, Cos=1), the books share the exact same keywords.\n",
    "-   If vectors are orthogonal (Angle=90, Cos=0), they share no keywords.\n",
    "\n",
    "## Code Block Summary\n",
    "1.  **Vectorization**: Converts the `tag_soup` column strings into a large sparse matrix (Numbers).\n",
    "2.  **Similarity Calculation**: Computes the Cosine Similarity between *every* pair of books in the dataset (10,000 x 10,000 matrix).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab54327c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting TF-IDF Vectorization...\n",
      "TF-IDF Matrix Shape: (10000, 1164)\n",
      "Calculating Cosine Similarity Matrix (This may take a moment)...\n",
      "Cosine Similarity Matrix Calculation Complete.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "print(\"Starting TF-IDF Vectorization...\")\n",
    "\n",
    "# 1. Initialize the TF-IDF Vectorizer\n",
    "# max_df=0.9: Ignore terms that appear in more than 90% of documents (too common)\n",
    "# min_df=5: Ignore terms that appear in fewer than 5 documents (too rare)\n",
    "tfidf = TfidfVectorizer(stop_words='english', max_df=0.9, min_df=5)\n",
    "\n",
    "# Fit the vectorizer to the 'tag_soup' and transform the text into a sparse matrix\n",
    "tfidf_matrix = tfidf.fit_transform(df['tag_soup'])\n",
    "\n",
    "print(f\"TF-IDF Matrix Shape: {tfidf_matrix.shape}\")\n",
    "print(\"Calculating Cosine Similarity Matrix (This may take a moment)...\")\n",
    "\n",
    "# 2. Compute the Cosine Similarity Matrix\n",
    "# The resulting matrix (N x N) holds the similarity score between every pair of books.\n",
    "# This matrix will be the core of your CBF model.\n",
    "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "print(\"Cosine Similarity Matrix Calculation Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ffa6a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping series to quickly lookup the internal index from the book_id\n",
    "book_to_index_series = pd.Series(df.index, index=df['book_id']).drop_duplicates()\n",
    "\n",
    "def recommend_cbf_by_book_id(book_id, cosine_sim_matrix, df_content, top_k=10):\n",
    "    \n",
    "    # 1. Get the internal index corresponding to the input book_id\n",
    "    try:\n",
    "        idx = book_to_index_series[book_id]\n",
    "    except KeyError:\n",
    "        return f\"Error: Book ID {book_id} not found in content data.\"\n",
    "\n",
    "    # 2. Get the similarity scores for this book against all other books\n",
    "    # The scores are the row/column corresponding to 'idx' in the matrix\n",
    "    sim_scores = list(enumerate(cosine_sim_matrix[idx]))\n",
    "\n",
    "    # 3. Sort the books based on the similarity scores (descending)\n",
    "    # We ignore the first element (index 0) because it's the book itself (similarity = 1.0)\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    sim_scores = sim_scores[1:top_k + 1] \n",
    "\n",
    "    # 4. Get the book indices and similarity scores\n",
    "    book_indices = [i[0] for i in sim_scores]\n",
    "    scores = [i[1] for i in sim_scores]\n",
    "    \n",
    "    # 5. Get the original book IDs and titles for the top recommendations\n",
    "    recommended_books = df_content.iloc[book_indices].copy()\n",
    "    \n",
    "    # 6. Format and return\n",
    "    results = pd.DataFrame({\n",
    "        'original_book_id': recommended_books['book_id'],\n",
    "        'cbf_score': scores,\n",
    "        'title': recommended_books['title'],\n",
    "        'authors': recommended_books['authors']\n",
    "    })\n",
    "    \n",
    "    return results.reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f56963d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating Content-Based Recommendations ---\n",
      "Anchor Book ID: 71\n",
      "Anchor Book Title: Frankenstein\n",
      "\n",
      "Results based on shared tags/authors (Content-Based Score):\n",
      "----------------------------------------------------------\n",
      "   cbf_score                                                                  title                                                            authors\n",
      "0   0.710223                                             The Picture of Dorian Gray                                     Oscar Wilde, Jeffrey Eugenides\n",
      "1   0.698998                            The Strange Case of Dr. Jekyll and Mr. Hyde  Robert Louis Stevenson, Vladimir Nabokov, Mervyn Peake, Dan Chaon\n",
      "2   0.624072                                                                Dracula                          Bram Stoker, Nina Auerbach, David J. Skal\n",
      "3   0.622619                                     Necronomicon: The Best Weird Tales                         H.P. Lovecraft, Les Edwards, Stephen Jones\n",
      "4   0.606474                                               The Island of Dr. Moreau                                                         H.G. Wells\n",
      "5   0.578673  The Strange Case of Dr. Jekyll and Mr. Hyde and Other Tales of Terror                             Robert Louis Stevenson, Robert Mighall\n",
      "6   0.554458                            The Legend of Sleepy Hollow (Graphic Novel)                      Bo Hampton, Tracey Hampton, Washington Irving\n",
      "7   0.546231                                                              The Pearl                                                     John Steinbeck\n",
      "8   0.544971                                                      The Invisible Man                                                         H.G. Wells\n",
      "9   0.542899                                                              The Other                                                       Thomas Tryon\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Assuming df, cosine_sim, and the function recommend_cbf_by_book_id are defined\n",
    "\n",
    "# --- 1. Select a Book ID to Use as the Anchor ---\n",
    "# We will use the first book ID found in your content data as a reliable test case.\n",
    "book_id_to_test = df['book_id'].iloc[70]\n",
    "\n",
    "# --- 2. Retrieve the Title of the Anchor Book (For context in the output) ---\n",
    "anchor_title = df[df['book_id'] == book_id_to_test]['title'].iloc[0]\n",
    "\n",
    "print(f\"--- Generating Content-Based Recommendations ---\")\n",
    "print(f\"Anchor Book ID: {book_id_to_test}\")\n",
    "print(f\"Anchor Book Title: {anchor_title}\\n\")\n",
    "\n",
    "# --- 3. Run the Recommendation Function ---\n",
    "recommendation_cbf = recommend_cbf_by_book_id(\n",
    "    book_id=book_id_to_test,\n",
    "    cosine_sim_matrix=cosine_sim,\n",
    "    df_content=df,\n",
    "    top_k=10 # Get the top 10 similar books\n",
    ")\n",
    "\n",
    "# --- 4. Display Results ---\n",
    "print(\"Results based on shared tags/authors (Content-Based Score):\")\n",
    "print(\"----------------------------------------------------------\")\n",
    "# Use to_string() for clean console display\n",
    "print(recommendation_cbf[['cbf_score', 'title', 'authors']].to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebe065a",
   "metadata": {},
   "source": [
    "The key challenge here is that the NCF function gives scores for all unseen items, while the CBF function only gives item-to-item similarity scores based on a single anchor book.\n",
    "\n",
    "For a true hybrid, we need to adapt the CBF logic to a User-Centric CBF Score:\n",
    "\n",
    "    Identify User's Favorite Books: Find the books the user rated highest.\n",
    "\n",
    "    Calculate Average Similarity: For every candidate book, calculate its average Content-Based Similarity to the user's favorite books. This gives us a Scbf​ score for that user/item pair.\n",
    "\n",
    "    Fuse Scores: Combine Sncf​ and Scbf​.\n",
    "\n",
    "1. Adaptation: User-Centric CBF Score (Required Pre-Calculation)\n",
    "\n",
    "First, we need a function that calculates the user-centric CBF score for all candidate books."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b2d93ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_user_cbf_scores(user_id, df_cf_input, df_positive, book_to_index_series, cosine_sim_matrix, candidate_book_indices):\n",
    "    \n",
    "    # 1. Identify the user's high-rated books (using internal row indices)\n",
    "    # We use df_positive (filtered high ratings) for a stronger signal\n",
    "    user_pos_ratings = df_positive[df_positive['user_id'] == user_id]\n",
    "    \n",
    "    # Get the row indices from df_books_content for these positive books\n",
    "    # Note: We are using the original book_id here to map to the content DataFrame index\n",
    "    user_fav_book_ids = user_pos_ratings['book_id'].unique()\n",
    "    \n",
    "    # Filter out any favorites that might not be in the content DataFrame\n",
    "    valid_fav_indices = [book_to_index_series[bid] for bid in user_fav_book_ids if bid in book_to_index_series]\n",
    "    \n",
    "    if not valid_fav_indices:\n",
    "        return pd.Series(0.0, index=candidate_book_indices) # Return 0 scores if user has no favorites\n",
    "\n",
    "    # 2. Calculate the average content similarity for all candidates\n",
    "    \n",
    "    # Extract the similarity scores for all candidate books across the rows of the user's favorites\n",
    "    # This gives us a (Num_Favorites x Num_Candidates) matrix\n",
    "    sims_to_favorites = cosine_sim_matrix[valid_fav_indices][:, candidate_book_indices]\n",
    "    \n",
    "    # Average the scores across the favorites axis (axis=0) to get one score per candidate book\n",
    "    avg_cbf_scores = np.mean(sims_to_favorites, axis=0)\n",
    "    \n",
    "    # Return as a pandas Series for easy merging later (indexed by candidate book index)\n",
    "    return pd.Series(avg_cbf_scores, index=candidate_book_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24fd2f4",
   "metadata": {},
   "source": [
    "2. The Final Hybrid Function\n",
    "\n",
    "This function puts the NCF score, the new CBF score, and the weighted fusion together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07804fb6",
   "metadata": {},
   "source": [
    "# 8. Hybrid Recommendation System\n",
    "\n",
    "## Theory: Weighted Fusion\n",
    "Different models excel at different things:\n",
    "-   **NCF** provides **Serendipity**: It finds books other users liked, even if they seem unrelated by content.\n",
    "-   **CBF** provides **Relevance**: It guarantees the recommendation is textually similar to what you already like.\n",
    "\n",
    "We combine them using a weighted sum:\n",
    "$$ Score_{Final} = 0.7 \\times Score_{NCF} + 0.3 \\times Score_{CBF} $$\n",
    "\n",
    "## Code Block Summary\n",
    "The `recommend_hybrid` function:\n",
    "1.  **NCF Score**: Gets the NCF probability for all unread books.\n",
    "2.  **CBF Score**: Calculates the average similarity of each unread book to the user's *existing favorites*.\n",
    "3.  **Merge**: Combines both scores using the weights defined above.\n",
    "4.  **Rank**: Returns the books with the highest combined hybrid score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "03b6ee17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_hybrid(user_id, ncf_model, df_cf_input, df_positive, book_to_index, book_to_index_series, n_books, cosine_sim_matrix, top_k=10, w_ncf=0.7, w_cbf=0.3):\n",
    "    \n",
    "    # --- Part A: Get NCF Scores and Candidate List ---\n",
    "    \n",
    "    # 1. Run the NCF prediction function (we adapt the logic slightly for efficiency)\n",
    "    # Re-use the logic from recommend_ncf_for_user to get the base predictions and candidates\n",
    "    \n",
    "    # Get user index\n",
    "    user_index = user_to_index.get(user_id)\n",
    "    if user_index is None:\n",
    "        return \"User not found.\"\n",
    "\n",
    "    # Identify candidates\n",
    "    user_interactions = df_cf_input[df_cf_input['user_id'] == user_id]['book_index'].unique()\n",
    "    all_book_indices = np.arange(n_books)\n",
    "    candidate_book_indices = np.setdiff1d(all_book_indices, user_interactions)\n",
    "    \n",
    "    # Run NCF prediction\n",
    "    user_indices_array = np.full(len(candidate_book_indices), user_index)\n",
    "    ncf_scores_array = ncf_model.predict([user_indices_array, candidate_book_indices], verbose=0).flatten()\n",
    "    \n",
    "    # Create base DataFrame\n",
    "    df_scores = pd.DataFrame({\n",
    "        'book_index': candidate_book_indices,\n",
    "        'ncf_score': ncf_scores_array\n",
    "    }).set_index('book_index')\n",
    "\n",
    "    \n",
    "    # --- Part B: Get CBF Scores ---\n",
    "\n",
    "    # 2. Calculate the User-Centric CBF Score for all candidates\n",
    "    cbf_scores_series = calculate_user_cbf_scores(\n",
    "        user_id, df_cf_input, df_positive, book_to_index_series, cosine_sim_matrix, candidate_book_indices\n",
    "    )\n",
    "    \n",
    "    # Merge CBF scores onto the main score DataFrame\n",
    "    df_scores['cbf_score'] = cbf_scores_series\n",
    "    \n",
    "    \n",
    "    # --- Part C: Fusion and Final Ranking ---\n",
    "    \n",
    "    # 3. Apply the Hybrid Fusion Formula\n",
    "    df_scores['hybrid_score'] = (w_ncf * df_scores['ncf_score']) + (w_cbf * df_scores['cbf_score'])\n",
    "    \n",
    "    # 4. Final Ranking\n",
    "    top_recommendations = df_scores.sort_values('hybrid_score', ascending=False).head(top_k)\n",
    "    \n",
    "    \n",
    "    # --- Part D: Mapping and Formatting ---\n",
    "    \n",
    "    # Create the inverse map (index_to_book)\n",
    "    index_to_book = {v: k for k, v in book_to_index.items()}\n",
    "    \n",
    "    # Map the internal index back to the external book_id\n",
    "    top_recommendations['original_book_id'] = [index_to_book[idx] for idx in top_recommendations.index]\n",
    "    \n",
    "    return top_recommendations.reset_index().drop(columns=['book_index'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fbca4d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating HYBRID Recommendations for User ID: 1 ---\n",
      "Fusion Weights: NCF=0.7, CBF=0.3\n",
      "\n",
      "Final Hybrid Score Breakdown (Top 10):\n",
      "--------------------------------------\n",
      " Hybrid Score  NCF (Taste)  CBF (Content)                                                                               Book Title                                                Authors  Avg Rating\n",
      "     0.773858     0.994915       0.258057                                                                         The Great Gatsby                                    F. Scott Fitzgerald        3.89\n",
      "     0.763351     0.980147       0.257495                                                             Night (The Night Trilogy #1)                             Elie Wiesel, Marion Wiesel        4.30\n",
      "     0.763179     0.981769       0.253135                                                              The World According to Garp                                            John Irving        4.07\n",
      "     0.756095     0.985880       0.219931 The Devil in the White City: Murder, Magic, and Madness at the Fair That Changed America                              Erik Larson, Tony Goldwyn        3.98\n",
      "     0.747272     0.978247       0.208332                                                                 Like Water for Chocolate Laura Esquivel, Thomas  Christensen, Carol Christensen        3.94\n",
      "     0.743007     0.965889       0.222951                                            Blink: The Power of Thinking Without Thinking                                       Malcolm Gladwell        3.89\n",
      "     0.741001     0.977259       0.189732                                                                          The Thorn Birds                                     Colleen McCullough        4.22\n",
      "     0.738832     0.975886       0.185704                                                                 I Know This Much Is True                                             Wally Lamb        4.17\n",
      "     0.738396     0.967576       0.203641                                   Dreams from My Father: A Story of Race and Inheritance                                           Barack Obama        3.81\n",
      "     0.737071     0.977883       0.175177                                                                  The Old Man and the Sea                                       Ernest Hemingway        3.73\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Assumes: ncf_model, df_cf_input, df_positive, book_to_index, book_to_index_series, \n",
    "# n_books, cosine_sim_matrix, and df_books_content are all defined.\n",
    "# Assumes: The recommend_hybrid and calculate_user_cbf_scores functions are defined.\n",
    "\n",
    "# --- 1. Find a Valid User ID to Test ---\n",
    "# We select a user who has positive ratings, ensuring both NCF and CBF signals are active.\n",
    "user_id_to_test = df_positive['user_id'].iloc[0] \n",
    "\n",
    "# --- 2. Define Fusion Weights ---\n",
    "# These are the hyperparameters that control the balance of the hybrid system.\n",
    "W_NCF = 0.7  # Weight for personalized taste (NCF)\n",
    "W_CBF = 0.3  # Weight for content relevance (CBF/TF-IDF)\n",
    "\n",
    "print(f\"--- Generating HYBRID Recommendations for User ID: {user_id_to_test} ---\")\n",
    "print(f\"Fusion Weights: NCF={W_NCF}, CBF={W_CBF}\\n\")\n",
    "\n",
    "# --- 3. Execute the Hybrid Recommendation Function ---\n",
    "final_recs_raw = recommend_hybrid(\n",
    "    user_id=user_id_to_test,\n",
    "    ncf_model=ncf_model,\n",
    "    df_cf_input=df_cf_input,\n",
    "    df_positive=df_positive,\n",
    "    book_to_index=book_to_index,\n",
    "    book_to_index_series=book_to_index_series,\n",
    "    n_books=n_books,\n",
    "    cosine_sim_matrix=cosine_sim,\n",
    "    top_k=10, \n",
    "    w_ncf=W_NCF, \n",
    "    w_cbf=W_CBF\n",
    ")\n",
    "\n",
    "# --- 4. Merge Raw Scores with Book Content for Readability ---\n",
    "# Join the raw results (which contain the calculated scores) with the main content DataFrame\n",
    "final_recs_display = pd.merge(\n",
    "    final_recs_raw,\n",
    "    df[['book_id', 'title', 'authors', 'average_rating']],\n",
    "    left_on='original_book_id',\n",
    "    right_on='book_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# --- 5. Display Final Results ---\n",
    "\n",
    "print(\"Final Hybrid Score Breakdown (Top 10):\")\n",
    "print(\"--------------------------------------\")\n",
    "\n",
    "# Select and rename columns for a clean output table\n",
    "output_columns = {\n",
    "    'hybrid_score': 'Hybrid Score',\n",
    "    'ncf_score': 'NCF (Taste)',\n",
    "    'cbf_score': 'CBF (Content)',\n",
    "    'title': 'Book Title',\n",
    "    'authors': 'Authors',\n",
    "    'average_rating': 'Avg Rating'\n",
    "}\n",
    "\n",
    "final_recs_display = final_recs_display.rename(columns=output_columns)\n",
    "\n",
    "# Display the final, beautifully organized table\n",
    "print(final_recs_display[[\n",
    "    'Hybrid Score',\n",
    "    'NCF (Taste)',\n",
    "    'CBF (Content)',\n",
    "    'Book Title',\n",
    "    'Authors',\n",
    "    'Avg Rating'\n",
    "]].to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
